{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **📌LinkedIn Job Scraper**"
      ],
      "metadata": {
        "id": "o8xI7I66heTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stable Chrome + Chromedriver Setup for Colab"
      ],
      "metadata": {
        "id": "eIT4EuMzRibn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Remove any old Chrome/Chromedriver\n",
        "rm -rf /usr/bin/chromedriver /usr/local/bin/chromedriver\n",
        "\n",
        "# Install Google Chrome\n",
        "wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "apt-get -y install ./google-chrome-stable_current_amd64.deb\n",
        "\n",
        "# Get Chrome version\n",
        "CHROME_VERSION=$(google-chrome --version | cut -d \" \" -f3 | cut -d \".\" -f1)\n",
        "echo \"Installed Chrome version: $CHROME_VERSION\"\n",
        "\n",
        "# Download matching ChromeDriver\n",
        "DRIVER_VERSION=$(wget -qO- \"https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_${CHROME_VERSION}\")\n",
        "echo \"Matching Driver version: $DRIVER_VERSION\"\n",
        "\n",
        "wget -q \"https://storage.googleapis.com/chrome-for-testing-public/${DRIVER_VERSION}/linux64/chromedriver-linux64.zip\"\n",
        "unzip -q chromedriver-linux64.zip\n",
        "mv chromedriver-linux64/chromedriver /usr/bin/chromedriver\n",
        "chmod +x /usr/bin/chromedriver\n",
        "\n",
        "# Verify installation\n",
        "google-chrome --version\n",
        "chromedriver --version\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCBKPE_zmmDE",
        "outputId": "b06d063f-ad92-4a73-cb55-e35f01e18989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  google-chrome-stable libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 10.9 MB/130 MB of archives.\n",
            "After this operation, 438 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.3 [10.7 MB]\n",
            "Get:3 /content/google-chrome-stable_current_amd64.deb google-chrome-stable amd64 139.0.7258.127-1 [119 MB]\n",
            "Fetched 10.9 MB in 1s (7,769 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126380 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\r\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\r\n",
            "Selecting previously unselected package google-chrome-stable.\r\n",
            "Preparing to unpack .../google-chrome-stable_current_amd64.deb ...\r\n",
            "Unpacking google-chrome-stable (139.0.7258.127-1) ...\r\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\r\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\r\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\r\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\r\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\r\n",
            "Setting up google-chrome-stable (139.0.7258.127-1) ...\r\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\r\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\r\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\r\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
            "\r\n",
            "Installed Chrome version: 139\n",
            "Matching Driver version: 139.0.7258.68\n",
            "Google Chrome 139.0.7258.127 \n",
            "ChromeDriver 139.0.7258.68 (40ff94600b6ed9fa7778a3a2566f254ad85f2147-refs/branch-heads/7258@{#2228})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Setup Chrome options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")   # Run headless (no GUI)\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Point to chromedriver\n",
        "service = Service(\"/usr/bin/chromedriver\")\n",
        "\n",
        "# Start browser\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "driver.get(\"https://www.google.com\")\n",
        "\n",
        "print(\"✅ Page title:\", driver.title)\n",
        "\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta53p9U5nlM2",
        "outputId": "074319d1-b582-4721-b3ba-9daf44dc69a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Page title: Google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQhxb7HqhJMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINALIZED"
      ],
      "metadata": {
        "id": "P16QxM73fhXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Dependencies: selenium>=4.18, webdriver-manager, undetected-chromedriver,\n",
        "# pandas, beautifulsoup4, lxml, tenacity, python-dateutil\n",
        "# -----------------------------------------------\n",
        "!pip install -q selenium==4.18.1 webdriver-manager undetected-chromedriver pandas beautifulsoup4 lxml tenacity python-dateutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1mv3HMTNCsv",
        "outputId": "72855b74-5b92-4fdb-af28-c09670c30c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for undetected-chromedriver (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "9D-IDg3-fjir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# ==================== CONFIG ====================\n",
        "# -----------------------------------------------\n",
        "\n",
        "CONFIG = {\n",
        "    \"keywords\": [\"Data Analyst\", \"Software Engineer\"],\n",
        "    \"locations\": [\"Pune, Maharashtra, India\", \"Bengaluru, Karnataka, India\"],\n",
        "    \"result_limit\": 30,           # Max jobs per keyword-location combo\n",
        "    \"fetch_descriptions\": False,  # Toggle to scrape full job descriptions\n",
        "\n",
        "    # --- FILTERS ---\n",
        "    \"filters\": {\n",
        "        \"remote\": None,           # options: None, \"1\" (remote), \"2\" (on-site), \"3\" (hybrid)\n",
        "        \"date_posted\": \"r86400\", # past week (None, r86400=24h, r604800=week, r2592000=month)\n",
        "        \"experience\": None,       # options: 1=Internship, 2=Entry, 3=Associate, etc.\n",
        "        \"employment_type\": None,  # options: 1=Full-time, 2=Part-time, etc.\n",
        "    },\n",
        "\n",
        "    \"dedupe_on\": [\"Link\"],        # Remove duplicates based on job link\n",
        "    \"save_chunk_size\": 50,        # Save after every N jobs\n",
        "    \"max_retries\": 3,             # Retry failed pages/cards N times\n",
        "    \"retry_delay\": 3,             # Seconds between retries\n",
        "\n",
        "    # --- HUMAN-LIKE DELAY ---\n",
        "    \"random_delay\": {\n",
        "        \"enabled\": False,          # True = wait random seconds, False = skip waiting\n",
        "        \"min_sec\": 2,             # Minimum seconds to wait\n",
        "        \"max_sec\": 6              # Maximum seconds to wait\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "VBd02ykfftR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# ==================== DRIVER SETUP ====================\n",
        "# -----------------------------------------------\n",
        "\n",
        "def setup_driver() -> uc.Chrome:\n",
        "    options = uc.ChromeOptions()\n",
        "    options.add_argument(\"--headless=new\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = uc.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver"
      ],
      "metadata": {
        "id": "UcEt0t8pfxA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# ==================== AUTH ====================\n",
        "# -----------------------------------------------\n",
        "\n",
        "def login_linkedin(driver):\n",
        "    li_at = os.getenv(\"LI_AT\")\n",
        "    username = os.getenv(\"LINKEDIN_USER\")\n",
        "    password = os.getenv(\"LINKEDIN_PASS\")\n",
        "\n",
        "    driver.get(\"https://www.linkedin.com/login\")\n",
        "\n",
        "    if li_at:\n",
        "        driver.add_cookie({\"name\": \"li_at\", \"value\": li_at, \"domain\": \".linkedin.com\"})\n",
        "        driver.refresh()\n",
        "        print(\"[INFO] Logged in with li_at cookie\")\n",
        "    elif username and password:\n",
        "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(username)\n",
        "        driver.find_element(By.ID, \"password\").send_keys(password)\n",
        "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
        "        print(\"[INFO] Logged in with username/password\")\n",
        "    else:\n",
        "        print(\"[WARN] No login → scraping public jobs only\")"
      ],
      "metadata": {
        "id": "7HTVHY-af0_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# ==================== URL BUILDER ====================\n",
        "# -----------------------------------------------\n",
        "\n",
        "def build_search_url(keyword: str, location: str, start: int = 0, filters: dict = None) -> str:\n",
        "    url = f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}\"\n",
        "    if filters:\n",
        "        if filters.get(\"remote\"): url += f\"&f_WT={filters['remote']}\"\n",
        "        if filters.get(\"date_posted\"): url += f\"&f_TPR={filters['date_posted']}\"\n",
        "        if filters.get(\"experience\"): url += f\"&f_E={filters['experience']}\"\n",
        "        if filters.get(\"employment_type\"): url += f\"&f_JT={filters['employment_type']}\"\n",
        "    return url"
      ],
      "metadata": {
        "id": "s7X70QNmf1yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# ==================== RANDOM DELAY ====================\n",
        "# -----------------------------------------------\n",
        "\n",
        "def apply_random_delay():\n",
        "    if CONFIG[\"random_delay\"][\"enabled\"]:\n",
        "        delay = random.uniform(CONFIG[\"random_delay\"][\"min_sec\"], CONFIG[\"random_delay\"][\"max_sec\"])\n",
        "        print(f\"[WAIT] Sleeping {delay:.2f} seconds (human-like delay)\")\n",
        "        time.sleep(delay)"
      ],
      "metadata": {
        "id": "n5zMojPuf1uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# ==================== JOB EXTRACTION ====================\n",
        "# -----------------------------------------------\n",
        "\n",
        "def extract_job_card(card, fetch_desc: bool, driver, retries=CONFIG[\"max_retries\"]) -> Dict:\n",
        "    \"\"\"Extracts job info from a single LinkedIn job card with retries\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
        "            company = card.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
        "            loc = card.find_element(By.CSS_SELECTOR, \"span.job-search-card__location\").text.strip()\n",
        "            link = card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\").split(\"?\")[0]\n",
        "            date_posted = card.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")\n",
        "\n",
        "            job = {\n",
        "                \"Title\": title,\n",
        "                \"Company\": company,\n",
        "                \"Location\": loc,\n",
        "                \"Date Posted\": date_posted,\n",
        "                \"Link\": link,\n",
        "                \"Scraped At\": datetime.utcnow().isoformat()\n",
        "            }\n",
        "\n",
        "            # Fetch job description if enabled\n",
        "            if fetch_desc:\n",
        "                try:\n",
        "                    card.click()\n",
        "                    WebDriverWait(driver, 5).until(\n",
        "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobs-description\"))\n",
        "                    )\n",
        "                    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "                    desc = soup.select_one(\"div.jobs-description__content\")\n",
        "                    if desc:\n",
        "                        job[\"Job Description\"] = desc.get_text(\" \", strip=True)\n",
        "                except Exception:\n",
        "                    job[\"Job Description\"] = None\n",
        "\n",
        "            return job\n",
        "        except Exception:\n",
        "            time.sleep(CONFIG[\"retry_delay\"])\n",
        "            print(f\"[WARN] Job extraction retry {attempt+1}/{retries}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "AkAENpg-f1sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# ==================== SCRAPER ====================\n",
        "# -----------------------------------------------\n",
        "\n",
        "def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool, filters: dict) -> List[Dict]:\n",
        "    jobs = []\n",
        "    seen_links = set()\n",
        "    start = 0\n",
        "\n",
        "    while len(jobs) < limit:\n",
        "        url = build_search_url(keyword, location, start, filters)\n",
        "\n",
        "        # Retry page load\n",
        "        for attempt in range(CONFIG[\"max_retries\"]):\n",
        "            try:\n",
        "                driver.get(url)\n",
        "                WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"ul.jobs-search__results-list li\"))\n",
        "                )\n",
        "                break\n",
        "            except Exception:\n",
        "                print(f\"[WARN] Page load failed → retry {attempt+1}/{CONFIG['max_retries']}\")\n",
        "                time.sleep(CONFIG[\"retry_delay\"])\n",
        "        else:\n",
        "            print(\"[ERROR] Skipping page due to repeated failures\")\n",
        "            break\n",
        "\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
        "        if not cards:\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            job = extract_job_card(card, fetch_desc, driver)\n",
        "            if job and job[\"Link\"] not in seen_links:\n",
        "                jobs.append(job)\n",
        "                seen_links.add(job[\"Link\"])\n",
        "\n",
        "                # Apply random human-like delay\n",
        "                apply_random_delay()\n",
        "\n",
        "            if len(jobs) >= limit:\n",
        "                break\n",
        "\n",
        "        start += len(cards)\n",
        "\n",
        "    return jobs"
      ],
      "metadata": {
        "id": "J_hekEBkgQRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# ==================== MAIN ====================\n",
        "# -----------------------------------------------\n",
        "\n",
        "def main(CONFIG):\n",
        "    driver = setup_driver()\n",
        "    login_linkedin(driver)\n",
        "\n",
        "    all_jobs = []\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"linkedin_jobs_{ts}.csv\"\n",
        "    json_path = f\"linkedin_jobs_{ts}.json\"\n",
        "\n",
        "    for kw in CONFIG[\"keywords\"]:\n",
        "        for loc in CONFIG[\"locations\"]:\n",
        "            jobs = scrape_jobs(driver, kw, loc, CONFIG[\"result_limit\"], CONFIG[\"fetch_descriptions\"], CONFIG[\"filters\"])\n",
        "            all_jobs.extend(jobs)\n",
        "            print(f\"[INFO] Fetched {len(jobs)} jobs for {kw} in {loc}\")\n",
        "\n",
        "            # Save incrementally\n",
        "            if len(all_jobs) >= CONFIG[\"save_chunk_size\"]:\n",
        "                df = pd.DataFrame(all_jobs)\n",
        "                if CONFIG[\"dedupe_on\"]:\n",
        "                    df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "                df.to_csv(csv_path, index=False)\n",
        "                df.to_json(json_path, orient=\"records\", indent=2)\n",
        "                print(f\"[AUTO-SAVE] Saved {len(df)} jobs → {csv_path}, {json_path}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    # Final save\n",
        "    df = pd.DataFrame(all_jobs)\n",
        "    if CONFIG[\"dedupe_on\"]:\n",
        "        df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    print(f\"[SUMMARY] Total scraped: {len(all_jobs)}, Unique after dedupe: {len(df)}\")\n",
        "    print(f\"Saved → {csv_path}, {json_path}\")\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(CONFIG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe9EbfyTgQES",
        "outputId": "3cffd86e-801e-4c00-aefb-912f2cfc1e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] No login → scraping public jobs only\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Pune, Maharashtra, India\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Bengaluru, Karnataka, India\n",
            "[AUTO-SAVE] Saved 60 jobs → linkedin_jobs_20250816_105146.csv, linkedin_jobs_20250816_105146.json\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Pune, Maharashtra, India\n",
            "[AUTO-SAVE] Saved 88 jobs → linkedin_jobs_20250816_105146.csv, linkedin_jobs_20250816_105146.json\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Bengaluru, Karnataka, India\n",
            "[AUTO-SAVE] Saved 118 jobs → linkedin_jobs_20250816_105146.csv, linkedin_jobs_20250816_105146.json\n",
            "[SUMMARY] Total scraped: 120, Unique after dedupe: 118\n",
            "Saved → linkedin_jobs_20250816_105146.csv, linkedin_jobs_20250816_105146.json\n",
            "                                               Title  \\\n",
            "0                                            Analyst   \n",
            "1                                          Associate   \n",
            "2                                       Data Analyst   \n",
            "3  Senior Analyst - Data Analytics - Pan India - ...   \n",
            "4                        Analyst, Clinical Analytics   \n",
            "\n",
            "                            Company                  Location Date Posted  \\\n",
            "0                      IntegriChain  Pune, Maharashtra, India  2025-08-15   \n",
            "1                         PwC India  Pune, Maharashtra, India  2025-08-15   \n",
            "2  GK HR Consulting India Pvt. Ltd.  Pune, Maharashtra, India  2025-08-16   \n",
            "3              Golden Opportunities  Pune, Maharashtra, India  2025-08-16   \n",
            "4                           Evolent  Pune, Maharashtra, India  2025-08-15   \n",
            "\n",
            "                                                Link  \\\n",
            "0  https://in.linkedin.com/jobs/view/analyst-at-i...   \n",
            "1  https://in.linkedin.com/jobs/view/associate-at...   \n",
            "2  https://in.linkedin.com/jobs/view/data-analyst...   \n",
            "3  https://in.linkedin.com/jobs/view/senior-analy...   \n",
            "4  https://in.linkedin.com/jobs/view/analyst-clin...   \n",
            "\n",
            "                   Scraped At  \n",
            "0  2025-08-16T10:51:49.465784  \n",
            "1  2025-08-16T10:51:49.534759  \n",
            "2  2025-08-16T10:51:49.599517  \n",
            "3  2025-08-16T10:51:49.664033  \n",
            "4  2025-08-16T10:51:49.724297  \n",
            "(118, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Author\n",
        "Hemant K  \n",
        "📧 hemant777.karpe@gmail.com\n",
        "🔗 [LinkedIn](https://www.linkedin.com/in/hemant-karpe)"
      ],
      "metadata": {
        "id": "DvR9NA4CF4iv"
      }
    }
  ]
}