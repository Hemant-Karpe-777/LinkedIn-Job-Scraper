{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stable Chrome + Chromedriver Setup for Colab"
      ],
      "metadata": {
        "id": "eIT4EuMzRibn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Remove any old Chrome/Chromedriver\n",
        "rm -rf /usr/bin/chromedriver /usr/local/bin/chromedriver\n",
        "\n",
        "# Install Google Chrome\n",
        "wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "apt-get -y install ./google-chrome-stable_current_amd64.deb\n",
        "\n",
        "# Get Chrome version\n",
        "CHROME_VERSION=$(google-chrome --version | cut -d \" \" -f3 | cut -d \".\" -f1)\n",
        "echo \"Installed Chrome version: $CHROME_VERSION\"\n",
        "\n",
        "# Download matching ChromeDriver\n",
        "DRIVER_VERSION=$(wget -qO- \"https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_${CHROME_VERSION}\")\n",
        "echo \"Matching Driver version: $DRIVER_VERSION\"\n",
        "\n",
        "wget -q \"https://storage.googleapis.com/chrome-for-testing-public/${DRIVER_VERSION}/linux64/chromedriver-linux64.zip\"\n",
        "unzip -q chromedriver-linux64.zip\n",
        "mv chromedriver-linux64/chromedriver /usr/bin/chromedriver\n",
        "chmod +x /usr/bin/chromedriver\n",
        "\n",
        "# Verify installation\n",
        "google-chrome --version\n",
        "chromedriver --version\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCBKPE_zmmDE",
        "outputId": "b06d063f-ad92-4a73-cb55-e35f01e18989"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  google-chrome-stable libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 10.9 MB/130 MB of archives.\n",
            "After this operation, 438 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.3 [10.7 MB]\n",
            "Get:3 /content/google-chrome-stable_current_amd64.deb google-chrome-stable amd64 139.0.7258.127-1 [119 MB]\n",
            "Fetched 10.9 MB in 1s (7,769 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126380 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\r\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\r\n",
            "Selecting previously unselected package google-chrome-stable.\r\n",
            "Preparing to unpack .../google-chrome-stable_current_amd64.deb ...\r\n",
            "Unpacking google-chrome-stable (139.0.7258.127-1) ...\r\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\r\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\r\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\r\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\r\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\r\n",
            "Setting up google-chrome-stable (139.0.7258.127-1) ...\r\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\r\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\r\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\r\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
            "\r\n",
            "Installed Chrome version: 139\n",
            "Matching Driver version: 139.0.7258.68\n",
            "Google Chrome 139.0.7258.127 \n",
            "ChromeDriver 139.0.7258.68 (40ff94600b6ed9fa7778a3a2566f254ad85f2147-refs/branch-heads/7258@{#2228})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Setup Chrome options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")   # Run headless (no GUI)\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Point to chromedriver\n",
        "service = Service(\"/usr/bin/chromedriver\")\n",
        "\n",
        "# Start browser\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "driver.get(\"https://www.google.com\")\n",
        "\n",
        "print(\"✅ Page title:\", driver.title)\n",
        "\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta53p9U5nlM2",
        "outputId": "074319d1-b582-4721-b3ba-9daf44dc69a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Page title: Google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code"
      ],
      "metadata": {
        "id": "B7Mk8tJelvek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 📌 LinkedIn Job Scraper\n",
        "# ================================================\n",
        "#\n",
        "# -----------------------------------------------\n",
        "# Dependencies: selenium>=4.18, webdriver-manager, undetected-chromedriver,\n",
        "# pandas, beautifulsoup4, lxml, tenacity, python-dateutil\n",
        "# -----------------------------------------------\n",
        "!pip install -q selenium==4.18.1 webdriver-manager undetected-chromedriver pandas beautifulsoup4 lxml tenacity python-dateutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1mv3HMTNCsv",
        "outputId": "72855b74-5b92-4fdb-af28-c09670c30c3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for undetected-chromedriver (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "O4osq56KlLJx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== CONFIG ====================\n",
        "CONFIG = {\n",
        "    \"keywords\": [\"Data Analyst\", \"Software Engineer\"],\n",
        "    \"locations\": [\"Pune, Maharashtra, India\",\"Bengaluru, Karnataka, India\"],\n",
        "    \"result_limit\": 30,          # Max jobs per keyword-location combo\n",
        "    \"fetch_descriptions\": False,  # Set True for full job descriptions\n",
        "    \"dedupe_on\": [\"Link\"]\n",
        "}"
      ],
      "metadata": {
        "id": "sLSD2WAklOpr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== DRIVER SETUP ====================\n",
        "def setup_driver() -> uc.Chrome:\n",
        "    options = uc.ChromeOptions()\n",
        "    options.add_argument(\"--headless=new\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = uc.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver"
      ],
      "metadata": {
        "id": "_-xwx8D6lQTn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== AUTH ====================\n",
        "def login_linkedin(driver):\n",
        "    li_at = os.getenv(\"LI_AT\")\n",
        "    username = os.getenv(\"LINKEDIN_USER\")\n",
        "    password = os.getenv(\"LINKEDIN_PASS\")\n",
        "\n",
        "    if li_at:\n",
        "        driver.get(\"https://www.linkedin.com\")\n",
        "        driver.add_cookie({\"name\": \"li_at\", \"value\": li_at, \"domain\": \".linkedin.com\"})\n",
        "        driver.refresh()\n",
        "        print(\"[INFO] Logged in with li_at cookie\")\n",
        "    elif username and password:\n",
        "        driver.get(\"https://www.linkedin.com/login\")\n",
        "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(username)\n",
        "        driver.find_element(By.ID, \"password\").send_keys(password)\n",
        "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
        "        print(\"[INFO] Logged in with username/password\")\n",
        "    else:\n",
        "        print(\"[WARN] No login provided → scraping public jobs only\")"
      ],
      "metadata": {
        "id": "ka-VEH3slTa2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== SCRAPER ====================\n",
        "def build_search_url(keyword: str, location: str, start: int = 0) -> str:\n",
        "    return (\n",
        "        f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}\"\n",
        "    )\n",
        "\n",
        "def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool) -> List[Dict]:\n",
        "    jobs = []\n",
        "    start = 0\n",
        "    while len(jobs) < limit:\n",
        "        url = build_search_url(keyword, location, start)\n",
        "        driver.get(url)\n",
        "        time.sleep(random.uniform(2, 4))\n",
        "\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
        "        if not cards:\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            try:\n",
        "                title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
        "                company = card.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
        "                loc = card.find_element(By.CSS_SELECTOR, \"span.job-search-card__location\").text.strip()\n",
        "                link = card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\").split(\"?\")[0]\n",
        "                date_posted = card.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")\n",
        "\n",
        "                job = {\n",
        "                    \"Title\": title,\n",
        "                    \"Company\": company,\n",
        "                    \"Location\": loc,\n",
        "                    \"Date Posted\": date_posted,\n",
        "                    \"Link\": link,\n",
        "                    \"Job Type\": None,\n",
        "                    \"Seniority\": None,\n",
        "                    \"Applicants\": None,\n",
        "                    \"Job Function\": None,\n",
        "                    \"Job Description\": None,\n",
        "                    \"Scraped At\": datetime.utcnow().isoformat()\n",
        "                }\n",
        "\n",
        "                if fetch_desc:\n",
        "                    try:\n",
        "                        card.click()\n",
        "                        WebDriverWait(driver, 5).until(\n",
        "                            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobs-description\"))\n",
        "                        )\n",
        "                        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "                        desc = soup.select_one(\"div.jobs-description__content\")\n",
        "                        job[\"Job Description\"] = desc.get_text(\" \", strip=True) if desc else None\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                jobs.append(job)\n",
        "                if len(jobs) >= limit:\n",
        "                    break\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        start += len(cards)\n",
        "        if len(cards) == 0:\n",
        "            break\n",
        "\n",
        "    return jobs"
      ],
      "metadata": {
        "id": "mOiIMIevlXDn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "WjWSj_xUk6Jf",
        "outputId": "0182bd69-e77a-45c8-acac-1d33389dcaa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] No login provided → scraping public jobs only\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Pune, Maharashtra, India\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Bengaluru, Karnataka, India\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Pune, Maharashtra, India\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Bengaluru, Karnataka, India\n",
            "[SUMMARY] Total scraped: 120, Unique after dedupe: 120\n",
            "Saved → linkedin_jobs_20250816_095000.csv, linkedin_jobs_20250816_095000.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                     Title                   Company  \\\n",
              "0             Data Analyst                        bp   \n",
              "1             Data Analyst                     Wipro   \n",
              "2             Data Analyst                  Michelin   \n",
              "3  Healthcare Data Analyst        Persistent Systems   \n",
              "4             Data Analyst  The Lubrizol Corporation   \n",
              "\n",
              "                   Location Date Posted  \\\n",
              "0  Pune, Maharashtra, India  2025-07-04   \n",
              "1  Pune, Maharashtra, India  2025-07-18   \n",
              "2  Pune, Maharashtra, India  2025-08-07   \n",
              "3  Pune, Maharashtra, India  2025-08-12   \n",
              "4  Pune, Maharashtra, India  2025-08-02   \n",
              "\n",
              "                                                Link Job Type Seniority  \\\n",
              "0  https://in.linkedin.com/jobs/view/data-analyst...     None      None   \n",
              "1  https://in.linkedin.com/jobs/view/data-analyst...     None      None   \n",
              "2  https://in.linkedin.com/jobs/view/data-analyst...     None      None   \n",
              "3  https://in.linkedin.com/jobs/view/healthcare-d...     None      None   \n",
              "4  https://in.linkedin.com/jobs/view/data-analyst...     None      None   \n",
              "\n",
              "  Applicants Job Function Job Description                  Scraped At  \n",
              "0       None         None            None  2025-08-16T09:49:40.864192  \n",
              "1       None         None            None  2025-08-16T09:49:40.914435  \n",
              "2       None         None            None  2025-08-16T09:49:40.963192  \n",
              "3       None         None            None  2025-08-16T09:49:41.017789  \n",
              "4       None         None            None  2025-08-16T09:49:41.084224  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ba53c5a-bd13-4b01-b274-2ac73f6fa53f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Company</th>\n",
              "      <th>Location</th>\n",
              "      <th>Date Posted</th>\n",
              "      <th>Link</th>\n",
              "      <th>Job Type</th>\n",
              "      <th>Seniority</th>\n",
              "      <th>Applicants</th>\n",
              "      <th>Job Function</th>\n",
              "      <th>Job Description</th>\n",
              "      <th>Scraped At</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>bp</td>\n",
              "      <td>Pune, Maharashtra, India</td>\n",
              "      <td>2025-07-04</td>\n",
              "      <td>https://in.linkedin.com/jobs/view/data-analyst...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-16T09:49:40.864192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>Wipro</td>\n",
              "      <td>Pune, Maharashtra, India</td>\n",
              "      <td>2025-07-18</td>\n",
              "      <td>https://in.linkedin.com/jobs/view/data-analyst...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-16T09:49:40.914435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>Michelin</td>\n",
              "      <td>Pune, Maharashtra, India</td>\n",
              "      <td>2025-08-07</td>\n",
              "      <td>https://in.linkedin.com/jobs/view/data-analyst...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-16T09:49:40.963192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Healthcare Data Analyst</td>\n",
              "      <td>Persistent Systems</td>\n",
              "      <td>Pune, Maharashtra, India</td>\n",
              "      <td>2025-08-12</td>\n",
              "      <td>https://in.linkedin.com/jobs/view/healthcare-d...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-16T09:49:41.017789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>The Lubrizol Corporation</td>\n",
              "      <td>Pune, Maharashtra, India</td>\n",
              "      <td>2025-08-02</td>\n",
              "      <td>https://in.linkedin.com/jobs/view/data-analyst...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-16T09:49:41.084224</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ba53c5a-bd13-4b01-b274-2ac73f6fa53f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1ba53c5a-bd13-4b01-b274-2ac73f6fa53f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1ba53c5a-bd13-4b01-b274-2ac73f6fa53f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fcc305e5-7dd9-4b72-8f18-fa14d8fa3cc5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fcc305e5-7dd9-4b72-8f18-fa14d8fa3cc5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fcc305e5-7dd9-4b72-8f18-fa14d8fa3cc5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120, 11)\n"
          ]
        }
      ],
      "source": [
        "# ==================== MAIN ====================\n",
        "def main(CONFIG):\n",
        "    driver = setup_driver()\n",
        "    login_linkedin(driver)\n",
        "\n",
        "    all_jobs = []\n",
        "    for kw in CONFIG[\"keywords\"]:\n",
        "        for loc in CONFIG[\"locations\"]:\n",
        "            jobs = scrape_jobs(driver, kw, loc, CONFIG[\"result_limit\"], CONFIG[\"fetch_descriptions\"])\n",
        "            all_jobs.extend(jobs)\n",
        "            print(f\"[INFO] Fetched {len(jobs)} jobs for {kw} in {loc}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    df = pd.DataFrame(all_jobs)\n",
        "    if CONFIG[\"dedupe_on\"]:\n",
        "        df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"linkedin_jobs_{ts}.csv\"\n",
        "    json_path = f\"linkedin_jobs_{ts}.json\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    print(f\"[SUMMARY] Total scraped: {len(all_jobs)}, Unique after dedupe: {len(df)}\")\n",
        "    print(f\"Saved → {csv_path}, {json_path}\")\n",
        "    display(df.head())\n",
        "    print(df.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(CONFIG)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "CONFIG = {\n",
        "    \"keywords\": [\"Data Analyst\", \"Software Engineer\"],\n",
        "    \"locations\": [\"Pune, Maharashtra, India\",\"Bengaluru, Karnataka, India\"],\n",
        "    \"result_limit\": 30,          # Max jobs per keyword-location combo\n",
        "    \"fetch_descriptions\": False,  # Set True for full job descriptions\n",
        "    \"dedupe_on\": [\"Link\"]\n",
        "}\n",
        "\n",
        "# ==================== DRIVER SETUP ====================\n",
        "def setup_driver() -> uc.Chrome:\n",
        "    options = uc.ChromeOptions()\n",
        "    options.add_argument(\"--headless=new\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = uc.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver\n",
        "\n",
        "# ==================== AUTH ====================\n",
        "def login_linkedin(driver):\n",
        "    li_at = os.getenv(\"LI_AT\")\n",
        "    username = os.getenv(\"LINKEDIN_USER\")\n",
        "    password = os.getenv(\"LINKEDIN_PASS\")\n",
        "\n",
        "    if li_at:\n",
        "        driver.get(\"https://www.linkedin.com\")\n",
        "        driver.add_cookie({\"name\": \"li_at\", \"value\": li_at, \"domain\": \".linkedin.com\"})\n",
        "        driver.refresh()\n",
        "        print(\"[INFO] Logged in with li_at cookie\")\n",
        "    elif username and password:\n",
        "        driver.get(\"https://www.linkedin.com/login\")\n",
        "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(username)\n",
        "        driver.find_element(By.ID, \"password\").send_keys(password)\n",
        "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
        "        print(\"[INFO] Logged in with username/password\")\n",
        "    else:\n",
        "        print(\"[WARN] No login provided → scraping public jobs only\")\n",
        "\n",
        "# ==================== SCRAPER ====================\n",
        "def build_search_url(keyword: str, location: str, start: int = 0) -> str:\n",
        "    return (\n",
        "        f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}\"\n",
        "    )\n",
        "\n",
        "def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool) -> List[Dict]:\n",
        "    jobs = []\n",
        "    start = 0\n",
        "    while len(jobs) < limit:\n",
        "        url = build_search_url(keyword, location, start)\n",
        "        driver.get(url)\n",
        "        time.sleep(random.uniform(2, 4))\n",
        "\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
        "        if not cards:\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            try:\n",
        "                title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
        "                company = card.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
        "                loc = card.find_element(By.CSS_SELECTOR, \"span.job-search-card__location\").text.strip()\n",
        "                link = card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\").split(\"?\")[0]\n",
        "                date_posted = card.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")\n",
        "\n",
        "                job = {\n",
        "                    \"Title\": title,\n",
        "                    \"Company\": company,\n",
        "                    \"Location\": loc,\n",
        "                    \"Date Posted\": date_posted,\n",
        "                    \"Link\": link,\n",
        "                    \"Job Type\": None,\n",
        "                    \"Seniority\": None,\n",
        "                    \"Applicants\": None,\n",
        "                    \"Job Function\": None,\n",
        "                    \"Job Description\": None,\n",
        "                    \"Scraped At\": datetime.utcnow().isoformat()\n",
        "                }\n",
        "\n",
        "                if fetch_desc:\n",
        "                    try:\n",
        "                        card.click()\n",
        "                        WebDriverWait(driver, 5).until(\n",
        "                            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobs-description\"))\n",
        "                        )\n",
        "                        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "                        desc = soup.select_one(\"div.jobs-description__content\")\n",
        "                        job[\"Job Description\"] = desc.get_text(\" \", strip=True) if desc else None\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                jobs.append(job)\n",
        "                if len(jobs) >= limit:\n",
        "                    break\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        start += len(cards)\n",
        "        if len(cards) == 0:\n",
        "            break\n",
        "\n",
        "    return jobs\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "def main(CONFIG):\n",
        "    driver = setup_driver()\n",
        "    login_linkedin(driver)\n",
        "\n",
        "    all_jobs = []\n",
        "    for kw in CONFIG[\"keywords\"]:\n",
        "        for loc in CONFIG[\"locations\"]:\n",
        "            jobs = scrape_jobs(driver, kw, loc, CONFIG[\"result_limit\"], CONFIG[\"fetch_descriptions\"])\n",
        "            all_jobs.extend(jobs)\n",
        "            print(f\"[INFO] Fetched {len(jobs)} jobs for {kw} in {loc}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    df = pd.DataFrame(all_jobs)\n",
        "    if CONFIG[\"dedupe_on\"]:\n",
        "        df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"linkedin_jobs_{ts}.csv\"\n",
        "    json_path = f\"linkedin_jobs_{ts}.json\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    print(f\"[SUMMARY] Total scraped: {len(all_jobs)}, Unique after dedupe: {len(df)}\")\n",
        "    print(f\"Saved → {csv_path}, {json_path}\")\n",
        "    display(df.head())\n",
        "    print(df.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(CONFIG)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "4GkGQLCLmXQN",
        "outputId": "ef4a259d-d1d0-4c0d-b441-f39754e93a89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] No login provided → scraping public jobs only\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Pune, Maharashtra, India\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Bengaluru, Karnataka, India\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Pune, Maharashtra, India\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Bengaluru, Karnataka, India\n",
            "[SUMMARY] Total scraped: 120, Unique after dedupe: 120\n",
            "Saved → linkedin_jobs_20250816_095104.csv, linkedin_jobs_20250816_095104.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                     Title                   Company  \\\n",
              "0             Data Analyst                        bp   \n",
              "1             Data Analyst                     Wipro   \n",
              "2             Data Analyst                  Michelin   \n",
              "3  Healthcare Data Analyst        Persistent Systems   \n",
              "4             Data Analyst  The Lubrizol Corporation   \n",
              "\n",
              "                   Location Date Posted  \\\n",
              "0  Pune, Maharashtra, India  2025-07-04   \n",
              "1  Pune, Maharashtra, India  2025-07-18   \n",
              "2  Pune, Maharashtra, India  2025-08-07   \n",
              "3  Pune, Maharashtra, India  2025-08-12   \n",
              "4  Pune, Maharashtra, India  2025-08-02   \n",
              "\n",
              "                                                Link Job Type Seniority  \\\n",
              "0  https://in.linkedin.com/jobs/view/data-analyst...     None      None   \n",
              "1  https://in.linkedin.com/jobs/view/data-analyst...     None      None   \n",
              "2  https://in.linkedin.com/jobs/view/data-analyst...     None      None   \n",
              "3  https://in.linkedin.com/jobs/view/healthcare-d...     None      None   \n",
              "4  https://in.linkedin.com/jobs/view/data-analyst...     None      None   \n",
              "\n",
              "  Applicants Job Function Job Description                  Scraped At  \n",
              "0       None         None            None  2025-08-16T09:50:43.287567  \n",
              "1       None         None            None  2025-08-16T09:50:43.337203  \n",
              "2       None         None            None  2025-08-16T09:50:43.389981  \n",
              "3       None         None            None  2025-08-16T09:50:43.439184  \n",
              "4       None         None            None  2025-08-16T09:50:43.501025  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c51e0ed4-493f-4df4-8b73-24d4133004a0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Company</th>\n",
              "      <th>Location</th>\n",
              "      <th>Date Posted</th>\n",
              "      <th>Link</th>\n",
              "      <th>Job Type</th>\n",
              "      <th>Seniority</th>\n",
              "      <th>Applicants</th>\n",
              "      <th>Job Function</th>\n",
              "      <th>Job Description</th>\n",
              "      <th>Scraped At</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>bp</td>\n",
              "      <td>Pune, Maharashtra, India</td>\n",
              "      <td>2025-07-04</td>\n",
              "      <td>https://in.linkedin.com/jobs/view/data-analyst...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-16T09:50:43.287567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>Wipro</td>\n",
              "      <td>Pune, Maharashtra, India</td>\n",
              "      <td>2025-07-18</td>\n",
              "      <td>https://in.linkedin.com/jobs/view/data-analyst...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-16T09:50:43.337203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>Michelin</td>\n",
              "      <td>Pune, Maharashtra, India</td>\n",
              "      <td>2025-08-07</td>\n",
              "      <td>https://in.linkedin.com/jobs/view/data-analyst...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-16T09:50:43.389981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Healthcare Data Analyst</td>\n",
              "      <td>Persistent Systems</td>\n",
              "      <td>Pune, Maharashtra, India</td>\n",
              "      <td>2025-08-12</td>\n",
              "      <td>https://in.linkedin.com/jobs/view/healthcare-d...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-16T09:50:43.439184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>The Lubrizol Corporation</td>\n",
              "      <td>Pune, Maharashtra, India</td>\n",
              "      <td>2025-08-02</td>\n",
              "      <td>https://in.linkedin.com/jobs/view/data-analyst...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-16T09:50:43.501025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c51e0ed4-493f-4df4-8b73-24d4133004a0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c51e0ed4-493f-4df4-8b73-24d4133004a0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c51e0ed4-493f-4df4-8b73-24d4133004a0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fac77a38-cfa1-4948-a9f8-a5cfd6bd4a85\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fac77a38-cfa1-4948-a9f8-a5cfd6bd4a85')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fac77a38-cfa1-4948-a9f8-a5cfd6bd4a85 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# try 1"
      ],
      "metadata": {
        "id": "4uUuyDGUUhqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "CONFIG = {\n",
        "    \"keywords\": [\"Data Analyst\", \"Software Engineer\"],\n",
        "    \"locations\": [\"Pune, Maharashtra, India\", \"Bengaluru, Karnataka, India\"],\n",
        "    \"result_limit\": 30,          # Max jobs per keyword-location combo\n",
        "    \"fetch_descriptions\": True,  # Fetch full job descriptions\n",
        "    \"dedupe_on\": [\"Link\"],\n",
        "    \"output_dir\": \"outputs\"\n",
        "}\n",
        "\n",
        "# ==================== LOGGER ====================\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    datefmt=\"%H:%M:%S\"\n",
        ")\n",
        "\n",
        "# ==================== DRIVER SETUP ====================\n",
        "def setup_driver() -> uc.Chrome:\n",
        "    options = uc.ChromeOptions()\n",
        "    options.add_argument(\"--headless=new\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = uc.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver\n",
        "\n",
        "# ==================== AUTH ====================\n",
        "def login_linkedin(driver):\n",
        "    li_at = os.getenv(\"LI_AT\")\n",
        "    username = os.getenv(\"LINKEDIN_USER\")\n",
        "    password = os.getenv(\"LINKEDIN_PASS\")\n",
        "\n",
        "    try:\n",
        "        if li_at:\n",
        "            driver.get(\"https://www.linkedin.com\")\n",
        "            driver.add_cookie({\"name\": \"li_at\", \"value\": li_at, \"domain\": \".linkedin.com\"})\n",
        "            driver.refresh()\n",
        "            logging.info(\"Logged in with li_at cookie ✅\")\n",
        "        elif username and password:\n",
        "            driver.get(\"https://www.linkedin.com/login\")\n",
        "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(username)\n",
        "            driver.find_element(By.ID, \"password\").send_keys(password)\n",
        "            driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
        "            logging.info(\"Logged in with username/password ✅\")\n",
        "        else:\n",
        "            logging.warning(\"No login provided → scraping public jobs only ⚠️\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Login failed: {e}\")\n",
        "\n",
        "# ==================== SCRAPER ====================\n",
        "def build_search_url(keyword: str, location: str, start: int = 0) -> str:\n",
        "    return f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}\"\n",
        "\n",
        "def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool) -> List[Dict]:\n",
        "    jobs = []\n",
        "    start = 0\n",
        "    while len(jobs) < limit:\n",
        "        url = build_search_url(keyword, location, start)\n",
        "        driver.get(url)\n",
        "\n",
        "        try:\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"ul.jobs-search__results-list li\"))\n",
        "            )\n",
        "        except:\n",
        "            logging.warning(\"No job cards found, stopping...\")\n",
        "            break\n",
        "\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
        "        if not cards:\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            try:\n",
        "                title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
        "                company = card.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
        "                loc = card.find_element(By.CSS_SELECTOR, \"span.job-search-card__location\").text.strip()\n",
        "                link = card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\").split(\"?\")[0]\n",
        "                date_posted = card.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")\n",
        "\n",
        "                job = {\n",
        "                    \"Title\": title,\n",
        "                    \"Company\": company,\n",
        "                    \"Location\": loc,\n",
        "                    \"Date Posted\": date_posted,\n",
        "                    \"Link\": link,\n",
        "                    \"Job Type\": None,\n",
        "                    \"Seniority\": None,\n",
        "                    \"Applicants\": None,\n",
        "                    \"Job Function\": None,\n",
        "                    \"Job Description\": None,\n",
        "                    \"Scraped At\": datetime.utcnow().isoformat()\n",
        "                }\n",
        "\n",
        "                if fetch_desc:\n",
        "                    try:\n",
        "                        card.click()\n",
        "                        WebDriverWait(driver, 5).until(\n",
        "                            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobs-description\"))\n",
        "                        )\n",
        "                        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "                        desc = soup.select_one(\"div.jobs-description__content\")\n",
        "                        job[\"Job Description\"] = desc.get_text(\" \", strip=True) if desc else None\n",
        "\n",
        "                        details = soup.select(\"ul.description__job-criteria-list li\")\n",
        "                        for d in details:\n",
        "                            label = d.select_one(\"h3\").text.strip()\n",
        "                            value = d.select_one(\"span\").text.strip()\n",
        "                            if \"Seniority\" in label: job[\"Seniority\"] = value\n",
        "                            if \"Employment\" in label: job[\"Job Type\"] = value\n",
        "                            if \"Job function\" in label: job[\"Job Function\"] = value\n",
        "                            if \"Applicants\" in label: job[\"Applicants\"] = value\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                jobs.append(job)\n",
        "                if len(jobs) >= limit:\n",
        "                    break\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        start += len(cards)\n",
        "        if len(cards) == 0:\n",
        "            break\n",
        "\n",
        "    return jobs\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "def main(CONFIG):\n",
        "    os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
        "\n",
        "    driver = setup_driver()\n",
        "    login_linkedin(driver)\n",
        "\n",
        "    all_jobs = []\n",
        "    for kw in CONFIG[\"keywords\"]:\n",
        "        for loc in CONFIG[\"locations\"]:\n",
        "            jobs = scrape_jobs(driver, kw, loc, CONFIG[\"result_limit\"], CONFIG[\"fetch_descriptions\"])\n",
        "            all_jobs.extend(jobs)\n",
        "            logging.info(f\"Fetched {len(jobs)} jobs for {kw} in {loc}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    df = pd.DataFrame(all_jobs)\n",
        "    if CONFIG[\"dedupe_on\"]:\n",
        "        df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = os.path.join(CONFIG[\"output_dir\"], f\"linkedin_jobs_{ts}.csv\")\n",
        "    json_path = os.path.join(CONFIG[\"output_dir\"], f\"linkedin_jobs_{ts}.json\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    logging.info(f\"SUMMARY → Total scraped: {len(all_jobs)}, Unique after dedupe: {len(df)}\")\n",
        "    logging.info(f\"Saved CSV → {csv_path}\")\n",
        "    logging.info(f\"Saved JSON → {json_path}\")\n",
        "\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(CONFIG)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJx85n17Ui_s",
        "outputId": "21f4fc11-d698-4e28-f7fb-fe7bc41a3ecc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No login provided → scraping public jobs only ⚠️\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Title             Company                  Location  \\\n",
            "0             Data Analyst                  bp  Pune, Maharashtra, India   \n",
            "1             Data Analyst               Wipro  Pune, Maharashtra, India   \n",
            "2             Data Analyst            Michelin  Pune, Maharashtra, India   \n",
            "3  Healthcare Data Analyst  Persistent Systems  Pune, Maharashtra, India   \n",
            "4                                                                          \n",
            "\n",
            "  Date Posted                                               Link Job Type  \\\n",
            "0  2025-07-04  https://in.linkedin.com/jobs/view/data-analyst...     None   \n",
            "1  2025-07-18  https://in.linkedin.com/jobs/view/data-analyst...     None   \n",
            "2  2025-08-07  https://in.linkedin.com/jobs/view/data-analyst...     None   \n",
            "3  2025-08-12  https://in.linkedin.com/jobs/view/healthcare-d...     None   \n",
            "4  2025-08-02  https://in.linkedin.com/jobs/view/data-analyst...     None   \n",
            "\n",
            "  Seniority Applicants Job Function Job Description  \\\n",
            "0      None       None         None            None   \n",
            "1      None       None         None            None   \n",
            "2      None       None         None            None   \n",
            "3      None       None         None            None   \n",
            "4      None       None         None            None   \n",
            "\n",
            "                   Scraped At  \n",
            "0  2025-08-16T10:00:09.768062  \n",
            "1  2025-08-16T10:00:10.941025  \n",
            "2  2025-08-16T10:00:12.058815  \n",
            "3  2025-08-16T10:00:13.168901  \n",
            "4  2025-08-16T10:00:14.289907  \n",
            "(120, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# try 2"
      ],
      "metadata": {
        "id": "SMzfVhnGVod2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import logging\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "CONFIG = {\n",
        "    \"keywords\": [\"Data Analyst\", \"Software Engineer\"],\n",
        "    \"locations\": [\"Pune, Maharashtra, India\", \"Bengaluru, Karnataka, India\"],\n",
        "    \"result_limit\": 30,           # Max jobs per keyword-location combo\n",
        "    \"fetch_descriptions\": True,   # Always fetch details\n",
        "    \"dedupe_on\": [\"Link\"]\n",
        "}\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "\n",
        "# ==================== DRIVER SETUP ====================\n",
        "def setup_driver() -> uc.Chrome:\n",
        "    options = uc.ChromeOptions()\n",
        "    options.add_argument(\"--headless=new\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = uc.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver\n",
        "\n",
        "# ==================== AUTH ====================\n",
        "def login_linkedin(driver):\n",
        "    li_at = os.getenv(\"LI_AT\")\n",
        "    username = os.getenv(\"LINKEDIN_USER\")\n",
        "    password = os.getenv(\"LINKEDIN_PASS\")\n",
        "\n",
        "    if li_at:\n",
        "        driver.get(\"https://www.linkedin.com\")\n",
        "        driver.add_cookie({\"name\": \"li_at\", \"value\": li_at, \"domain\": \".linkedin.com\"})\n",
        "        driver.refresh()\n",
        "        logging.info(\"Logged in with li_at cookie\")\n",
        "    elif username and password:\n",
        "        driver.get(\"https://www.linkedin.com/login\")\n",
        "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(username)\n",
        "        driver.find_element(By.ID, \"password\").send_keys(password)\n",
        "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
        "        logging.info(\"Logged in with username/password\")\n",
        "    else:\n",
        "        logging.warning(\"No login provided → scraping public jobs only\")\n",
        "\n",
        "# ==================== SCRAPER ====================\n",
        "def build_search_url(keyword: str, location: str, start: int = 0) -> str:\n",
        "    return f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}\"\n",
        "\n",
        "def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool) -> List[Dict]:\n",
        "    jobs = []\n",
        "    start = 0\n",
        "    while len(jobs) < limit:\n",
        "        url = build_search_url(keyword, location, start)\n",
        "        driver.get(url)\n",
        "        time.sleep(random.uniform(1.5, 2.5))\n",
        "\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
        "        if not cards:\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            try:\n",
        "                title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
        "                company = card.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
        "                loc = card.find_element(By.CSS_SELECTOR, \"span.job-search-card__location\").text.strip()\n",
        "                link = card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\").split(\"?\")[0]\n",
        "                date_posted = card.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")\n",
        "\n",
        "                job = {\n",
        "                    \"Title\": title,\n",
        "                    \"Company\": company,\n",
        "                    \"Location\": loc,\n",
        "                    \"Date Posted\": date_posted,\n",
        "                    \"Link\": link,\n",
        "                    \"Job Type\": \"\",\n",
        "                    \"Seniority\": \"\",\n",
        "                    \"Applicants\": \"\",\n",
        "                    \"Job Function\": \"\",\n",
        "                    \"Job Description\": \"\",\n",
        "                    \"Scraped At\": datetime.utcnow().isoformat()\n",
        "                }\n",
        "\n",
        "                if fetch_desc:\n",
        "                    try:\n",
        "                        driver.execute_script(\"arguments[0].click();\", card)\n",
        "                        WebDriverWait(driver, 6).until(\n",
        "                            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobs-details\"))\n",
        "                        )\n",
        "                        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "\n",
        "                        # Description\n",
        "                        desc = soup.select_one(\"div.jobs-description__content\")\n",
        "                        job[\"Job Description\"] = desc.get_text(\" \", strip=True) if desc else \"\"\n",
        "\n",
        "                        # Job criteria (Seniority, Employment, Function, Applicants)\n",
        "                        for li in soup.select(\"ul.description__job-criteria-list li\"):\n",
        "                            label = li.select_one(\"h3\")\n",
        "                            value = li.select_one(\"span\")\n",
        "                            if not label or not value:\n",
        "                                continue\n",
        "                            label = label.get_text(strip=True)\n",
        "                            value = value.get_text(strip=True)\n",
        "                            if \"Seniority\" in label:\n",
        "                                job[\"Seniority\"] = value\n",
        "                            elif \"Employment\" in label:\n",
        "                                job[\"Job Type\"] = value\n",
        "                            elif \"Job function\" in label:\n",
        "                                job[\"Job Function\"] = value\n",
        "                            elif \"Applicants\" in label:\n",
        "                                job[\"Applicants\"] = value\n",
        "                    except Exception as e:\n",
        "                        logging.debug(f\"Detail extraction failed: {e}\")\n",
        "\n",
        "                jobs.append(job)\n",
        "                logging.info(f\"Scraped: {title} | {company} | {loc} | {date_posted}\")\n",
        "                if len(jobs) >= limit:\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                logging.debug(f\"Card parse failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        start += len(cards)\n",
        "        if len(cards) == 0:\n",
        "            break\n",
        "\n",
        "    return jobs\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "def main(CONFIG):\n",
        "    driver = setup_driver()\n",
        "    login_linkedin(driver)\n",
        "\n",
        "    all_jobs = []\n",
        "    for kw in CONFIG[\"keywords\"]:\n",
        "        for loc in CONFIG[\"locations\"]:\n",
        "            jobs = scrape_jobs(driver, kw, loc, CONFIG[\"result_limit\"], CONFIG[\"fetch_descriptions\"])\n",
        "            all_jobs.extend(jobs)\n",
        "            logging.info(f\"Fetched {len(jobs)} jobs for {kw} in {loc}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    df = pd.DataFrame(all_jobs).fillna(\"\")\n",
        "    if CONFIG[\"dedupe_on\"]:\n",
        "        df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"linkedin_jobs_{ts}.csv\"\n",
        "    json_path = f\"linkedin_jobs_{ts}.json\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    logging.info(f\"SUMMARY → Total scraped: {len(all_jobs)}, Unique after dedupe: {len(df)}\")\n",
        "    logging.info(f\"Saved → {csv_path}, {json_path}\")\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(CONFIG)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fczob_ASVq4F",
        "outputId": "557ebff5-f257-43f8-e733-16f76b1ce36b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No login provided → scraping public jobs only\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Title             Company                  Location  \\\n",
            "0             Data Analyst                  bp  Pune, Maharashtra, India   \n",
            "1             Data Analyst               Wipro  Pune, Maharashtra, India   \n",
            "2             Data Analyst            Michelin  Pune, Maharashtra, India   \n",
            "3  Healthcare Data Analyst  Persistent Systems  Pune, Maharashtra, India   \n",
            "4                                                                          \n",
            "\n",
            "  Date Posted                                               Link Job Type  \\\n",
            "0  2025-07-04  https://in.linkedin.com/jobs/view/data-analyst...            \n",
            "1  2025-07-18  https://in.linkedin.com/jobs/view/data-analyst...            \n",
            "2  2025-08-07  https://in.linkedin.com/jobs/view/data-analyst...            \n",
            "3  2025-08-12  https://in.linkedin.com/jobs/view/healthcare-d...            \n",
            "4  2025-08-02  https://in.linkedin.com/jobs/view/data-analyst...            \n",
            "\n",
            "  Seniority Applicants Job Function Job Description  \\\n",
            "0                                                     \n",
            "1                                                     \n",
            "2                                                     \n",
            "3                                                     \n",
            "4                                                     \n",
            "\n",
            "                   Scraped At  \n",
            "0  2025-08-16T10:05:00.114093  \n",
            "1  2025-08-16T10:05:06.348878  \n",
            "2  2025-08-16T10:05:12.567848  \n",
            "3  2025-08-16T10:05:18.759381  \n",
            "4  2025-08-16T10:05:24.955651  \n",
            "(120, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# try 3"
      ],
      "metadata": {
        "id": "4NDFWYjEZQjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import logging\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "CONFIG = {\n",
        "    \"keywords\": [\"Data Analyst\", \"Software Engineer\"],\n",
        "    \"locations\": [\"Pune, Maharashtra, India\", \"Bengaluru, Karnataka, India\"],\n",
        "    \"result_limit\": 30,           # Max jobs per keyword-location combo\n",
        "    \"fetch_descriptions\": False,   # Toggle: fetch job descriptions (slower but richer data)\n",
        "    \"dedupe_on\": [\"Link\"],\n",
        "\n",
        "    #Filters\n",
        "    \"filters\": {\n",
        "        \"remote\": '2',             # None, \"1\" (remote), \"2\" (on-site), \"3\" (hybrid)\n",
        "        \"date_posted\": 'r86400',        # None, \"r86400\" (past 24h), \"r604800\" (past week), \"r2592000\" (past month)\n",
        "        \"experience\": None,         # 1=Internship, 2=Entry level, 3=Associate, 4=Mid-Senior, 5=Director, 6=Executive\n",
        "        \"employment_type\": None     # 1=Full-time, 2=Part-time, 3=Contract, 4=Temporary, 5=Volunteer, 6=Internship, 7=Other\n",
        "    }\n",
        "}\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "\n",
        "# ==================== DRIVER SETUP ====================\n",
        "def setup_driver() -> uc.Chrome:\n",
        "    options = uc.ChromeOptions()\n",
        "    options.add_argument(\"--headless=new\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = uc.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver\n",
        "\n",
        "# ==================== AUTH ====================\n",
        "def login_linkedin(driver):\n",
        "    li_at = os.getenv(\"LI_AT\")\n",
        "    username = os.getenv(\"LINKEDIN_USER\")\n",
        "    password = os.getenv(\"LINKEDIN_PASS\")\n",
        "\n",
        "    if li_at:\n",
        "        driver.get(\"https://www.linkedin.com\")\n",
        "        driver.add_cookie({\"name\": \"li_at\", \"value\": li_at, \"domain\": \".linkedin.com\"})\n",
        "        driver.refresh()\n",
        "        logging.info(\"Logged in with li_at cookie\")\n",
        "    elif username and password:\n",
        "        driver.get(\"https://www.linkedin.com/login\")\n",
        "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(username)\n",
        "        driver.find_element(By.ID, \"password\").send_keys(password)\n",
        "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
        "        logging.info(\"Logged in with username/password\")\n",
        "    else:\n",
        "        logging.warning(\"No login provided → scraping public jobs only\")\n",
        "\n",
        "# ==================== SCRAPER ====================\n",
        "def build_search_url(keyword: str, location: str, start: int = 0, filters: dict = None) -> str:\n",
        "    base = f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}\"\n",
        "    if filters:\n",
        "        if filters.get(\"remote\"):\n",
        "            base += f\"&f_WT={filters['remote']}\"\n",
        "        if filters.get(\"date_posted\"):\n",
        "            base += f\"&f_TPR={filters['date_posted']}\"\n",
        "        if filters.get(\"experience\"):\n",
        "            base += f\"&f_E={filters['experience']}\"\n",
        "        if filters.get(\"employment_type\"):\n",
        "            base += f\"&f_JT={filters['employment_type']}\"\n",
        "    return base\n",
        "\n",
        "def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool, filters: dict) -> List[Dict]:\n",
        "    jobs = []\n",
        "    start = 0\n",
        "    while len(jobs) < limit:\n",
        "        url = build_search_url(keyword, location, start, filters)\n",
        "        driver.get(url)\n",
        "        time.sleep(random.uniform(1.5, 2.5))\n",
        "\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
        "        if not cards:\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            try:\n",
        "                title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
        "                company = card.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
        "                loc = card.find_element(By.CSS_SELECTOR, \"span.job-search-card__location\").text.strip()\n",
        "                link = card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\").split(\"?\")[0]\n",
        "                date_posted = card.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")\n",
        "\n",
        "                job = {\n",
        "                    \"Title\": title,\n",
        "                    \"Company\": company,\n",
        "                    \"Location\": loc,\n",
        "                    \"Date Posted\": date_posted,\n",
        "                    \"Link\": link,\n",
        "                    \"Job Type\": \"\",\n",
        "                    \"Seniority\": \"\",\n",
        "                    \"Applicants\": \"\",\n",
        "                    \"Job Function\": \"\",\n",
        "                    \"Job Description\": \"\",\n",
        "                    \"Scraped At\": datetime.utcnow().isoformat()\n",
        "                }\n",
        "\n",
        "                if fetch_desc:\n",
        "                    try:\n",
        "                        driver.execute_script(\"arguments[0].click();\", card)\n",
        "                        WebDriverWait(driver, 6).until(\n",
        "                            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobs-details\"))\n",
        "                        )\n",
        "                        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "\n",
        "                        # Description\n",
        "                        desc = soup.select_one(\"div.jobs-description__content\")\n",
        "                        job[\"Job Description\"] = desc.get_text(\" \", strip=True) if desc else \"\"\n",
        "\n",
        "                        # Job criteria (Seniority, Employment, Function, Applicants)\n",
        "                        for li in soup.select(\"ul.description__job-criteria-list li\"):\n",
        "                            label = li.select_one(\"h3\")\n",
        "                            value = li.select_one(\"span\")\n",
        "                            if not label or not value:\n",
        "                                continue\n",
        "                            label = label.get_text(strip=True)\n",
        "                            value = value.get_text(strip=True)\n",
        "                            if \"Seniority\" in label:\n",
        "                                job[\"Seniority\"] = value\n",
        "                            elif \"Employment\" in label:\n",
        "                                job[\"Job Type\"] = value\n",
        "                            elif \"Job function\" in label:\n",
        "                                job[\"Job Function\"] = value\n",
        "                            elif \"Applicants\" in label:\n",
        "                                job[\"Applicants\"] = value\n",
        "                    except Exception as e:\n",
        "                        logging.debug(f\"Detail extraction failed: {e}\")\n",
        "\n",
        "                jobs.append(job)\n",
        "                logging.info(f\"Scraped: {title} | {company} | {loc} | {date_posted}\")\n",
        "                if len(jobs) >= limit:\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                logging.debug(f\"Card parse failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        start += len(cards)\n",
        "        if len(cards) == 0:\n",
        "            break\n",
        "\n",
        "    return jobs\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "def main(CONFIG):\n",
        "    driver = setup_driver()\n",
        "    login_linkedin(driver)\n",
        "\n",
        "    all_jobs = []\n",
        "    for kw in CONFIG[\"keywords\"]:\n",
        "        for loc in CONFIG[\"locations\"]:\n",
        "            jobs = scrape_jobs(driver, kw, loc, CONFIG[\"result_limit\"], CONFIG[\"fetch_descriptions\"], CONFIG[\"filters\"])\n",
        "            all_jobs.extend(jobs)\n",
        "            logging.info(f\"Fetched {len(jobs)} jobs for {kw} in {loc}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    df = pd.DataFrame(all_jobs).fillna(\"\")\n",
        "    if CONFIG[\"dedupe_on\"]:\n",
        "        df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"linkedin_jobs_{ts}.csv\"\n",
        "    json_path = f\"linkedin_jobs_{ts}.json\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    logging.info(f\"SUMMARY → Total scraped: {len(all_jobs)}, Unique after dedupe: {len(df)}\")\n",
        "    logging.info(f\"Saved → {csv_path}, {json_path}\")\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(CONFIG)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lyT0ttbZSDZ",
        "outputId": "42197711-6ce4-42df-ad5a-7b9078056af2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No login provided → scraping public jobs only\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title       Company  \\\n",
            "0                                     Data Scientist  Fusemachines   \n",
            "1                                Azure Data Engineer    Bridgenext   \n",
            "2       Director - Data Governance and AI Operations     SailPoint   \n",
            "3  Freelance Software Developer (Python) - Qualit...      Mindrift   \n",
            "4  Senior Software Engineer (Python/Go - Backend)...                 \n",
            "\n",
            "                   Location Date Posted  \\\n",
            "0  Pune, Maharashtra, India  2025-08-15   \n",
            "1  Pune, Maharashtra, India  2025-08-15   \n",
            "2  Pune, Maharashtra, India  2025-08-16   \n",
            "3  Pune, Maharashtra, India  2025-08-16   \n",
            "4                            2025-08-15   \n",
            "\n",
            "                                                Link Job Type Seniority  \\\n",
            "0  https://in.linkedin.com/jobs/view/data-scienti...                      \n",
            "1  https://in.linkedin.com/jobs/view/azure-data-e...                      \n",
            "2  https://in.linkedin.com/jobs/view/director-dat...                      \n",
            "3  https://in.linkedin.com/jobs/view/freelance-so...                      \n",
            "4  https://in.linkedin.com/jobs/view/senior-softw...                      \n",
            "\n",
            "  Applicants Job Function Job Description                  Scraped At  \n",
            "0                                          2025-08-16T10:21:46.264133  \n",
            "1                                          2025-08-16T10:21:46.344016  \n",
            "2                                          2025-08-16T10:21:46.426449  \n",
            "3                                          2025-08-16T10:21:46.518003  \n",
            "4                                          2025-08-16T10:21:46.602354  \n",
            "(26, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# try 4"
      ],
      "metadata": {
        "id": "_BSd6BKnbRNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "CONFIG = {\n",
        "    \"keywords\": [\"Data Analyst\", \"Software Engineer\"],\n",
        "    \"locations\": [\"Pune, Maharashtra, India\", \"Bengaluru, Karnataka, India\"],\n",
        "    \"result_limit\": 30,          # Max jobs per keyword-location combo\n",
        "    \"fetch_descriptions\": False, # Toggle to scrape full job descriptions\n",
        "\n",
        "    # --- NEW FILTERS ---\n",
        "    \"filters\": {\n",
        "        \"remote\": None,          # options: None, \"1\" (remote), \"2\" (on-site), \"3\" (hybrid)\n",
        "        \"date_posted\": None,     # options: None, \"r86400\" (past 24h), \"r604800\" (past week), etc.\n",
        "        \"experience\": None,      # options: 1=Internship, 2=Entry, 3=Associate, etc.\n",
        "        \"employment_type\": None, # options: 1=Full-time, 2=Part-time, etc.\n",
        "    },\n",
        "\n",
        "    \"dedupe_on\": [\"Link\"],       # Remove duplicates based on job link\n",
        "    \"save_chunk_size\": 50        # Save after every N jobs\n",
        "}\n",
        "\n",
        "# ==================== DRIVER SETUP ====================\n",
        "def setup_driver() -> uc.Chrome:\n",
        "    options = uc.ChromeOptions()\n",
        "    options.add_argument(\"--headless=new\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = uc.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver\n",
        "\n",
        "# ==================== AUTH ====================\n",
        "def login_linkedin(driver):\n",
        "    li_at = os.getenv(\"LI_AT\")\n",
        "    username = os.getenv(\"LINKEDIN_USER\")\n",
        "    password = os.getenv(\"LINKEDIN_PASS\")\n",
        "\n",
        "    driver.get(\"https://www.linkedin.com/login\")\n",
        "\n",
        "    if li_at:\n",
        "        driver.add_cookie({\"name\": \"li_at\", \"value\": li_at, \"domain\": \".linkedin.com\"})\n",
        "        driver.refresh()\n",
        "        print(\"[INFO] Logged in with li_at cookie\")\n",
        "    elif username and password:\n",
        "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(username)\n",
        "        driver.find_element(By.ID, \"password\").send_keys(password)\n",
        "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
        "        print(\"[INFO] Logged in with username/password\")\n",
        "    else:\n",
        "        print(\"[WARN] No login → scraping public jobs only\")\n",
        "\n",
        "# ==================== URL BUILDER ====================\n",
        "def build_search_url(keyword: str, location: str, start: int = 0, filters: dict = None) -> str:\n",
        "    url = f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}\"\n",
        "    if filters:\n",
        "        if filters.get(\"remote\"): url += f\"&f_WT={filters['remote']}\"\n",
        "        if filters.get(\"date_posted\"): url += f\"&f_TPR={filters['date_posted']}\"\n",
        "        if filters.get(\"experience\"): url += f\"&f_E={filters['experience']}\"\n",
        "        if filters.get(\"employment_type\"): url += f\"&f_JT={filters['employment_type']}\"\n",
        "    return url\n",
        "\n",
        "# ==================== JOB EXTRACTION ====================\n",
        "def extract_job_card(card, fetch_desc: bool, driver) -> Dict:\n",
        "    \"\"\"Extracts job info from a single LinkedIn job card\"\"\"\n",
        "    try:\n",
        "        title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
        "        company = card.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
        "        loc = card.find_element(By.CSS_SELECTOR, \"span.job-search-card__location\").text.strip()\n",
        "        link = card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\").split(\"?\")[0]\n",
        "        date_posted = card.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")\n",
        "\n",
        "        job = {\n",
        "            \"Title\": title,\n",
        "            \"Company\": company,\n",
        "            \"Location\": loc,\n",
        "            \"Date Posted\": date_posted,\n",
        "            \"Link\": link,\n",
        "            \"Scraped At\": datetime.utcnow().isoformat()\n",
        "        }\n",
        "\n",
        "        # Fetch job description if enabled\n",
        "        if fetch_desc:\n",
        "            try:\n",
        "                card.click()\n",
        "                WebDriverWait(driver, 5).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobs-description\"))\n",
        "                )\n",
        "                soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "                desc = soup.select_one(\"div.jobs-description__content\")\n",
        "                if desc:\n",
        "                    job[\"Job Description\"] = desc.get_text(\" \", strip=True)\n",
        "            except Exception:\n",
        "                job[\"Job Description\"] = None\n",
        "\n",
        "        return job\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ==================== SCRAPER ====================\n",
        "def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool, filters: dict) -> List[Dict]:\n",
        "    jobs = []\n",
        "    seen_links = set()\n",
        "    start = 0\n",
        "\n",
        "    while len(jobs) < limit:\n",
        "        url = build_search_url(keyword, location, start, filters)\n",
        "        driver.get(url)\n",
        "\n",
        "        # Smart wait instead of fixed sleep\n",
        "        WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"ul.jobs-search__results-list li\"))\n",
        "        )\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
        "        if not cards:\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            job = extract_job_card(card, fetch_desc, driver)\n",
        "            if job and job[\"Link\"] not in seen_links:\n",
        "                jobs.append(job)\n",
        "                seen_links.add(job[\"Link\"])\n",
        "\n",
        "            if len(jobs) >= limit:\n",
        "                break\n",
        "\n",
        "        start += len(cards)\n",
        "\n",
        "    return jobs\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "def main(CONFIG):\n",
        "    driver = setup_driver()\n",
        "    login_linkedin(driver)\n",
        "\n",
        "    all_jobs = []\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"linkedin_jobs_{ts}.csv\"\n",
        "    json_path = f\"linkedin_jobs_{ts}.json\"\n",
        "\n",
        "    for kw in CONFIG[\"keywords\"]:\n",
        "        for loc in CONFIG[\"locations\"]:\n",
        "            jobs = scrape_jobs(driver, kw, loc, CONFIG[\"result_limit\"], CONFIG[\"fetch_descriptions\"], CONFIG[\"filters\"])\n",
        "            all_jobs.extend(jobs)\n",
        "            print(f\"[INFO] Fetched {len(jobs)} jobs for {kw} in {loc}\")\n",
        "\n",
        "            # Save incrementally\n",
        "            if len(all_jobs) >= CONFIG[\"save_chunk_size\"]:\n",
        "                df = pd.DataFrame(all_jobs)\n",
        "                if CONFIG[\"dedupe_on\"]:\n",
        "                    df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "                df.to_csv(csv_path, index=False)\n",
        "                df.to_json(json_path, orient=\"records\", indent=2)\n",
        "                print(f\"[AUTO-SAVE] Saved {len(df)} jobs → {csv_path}, {json_path}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    # Final save\n",
        "    df = pd.DataFrame(all_jobs)\n",
        "    if CONFIG[\"dedupe_on\"]:\n",
        "        df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    print(f\"[SUMMARY] Total scraped: {len(all_jobs)}, Unique after dedupe: {len(df)}\")\n",
        "    print(f\"Saved → {csv_path}, {json_path}\")\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(CONFIG)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QygXNwMbYCL",
        "outputId": "97323d85-453d-4f9c-a79f-e88ae6dad961"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] No login → scraping public jobs only\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Pune, Maharashtra, India\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Bengaluru, Karnataka, India\n",
            "[AUTO-SAVE] Saved 60 jobs → linkedin_jobs_20250816_102959.csv, linkedin_jobs_20250816_102959.json\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Pune, Maharashtra, India\n",
            "[AUTO-SAVE] Saved 90 jobs → linkedin_jobs_20250816_102959.csv, linkedin_jobs_20250816_102959.json\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Bengaluru, Karnataka, India\n",
            "[AUTO-SAVE] Saved 120 jobs → linkedin_jobs_20250816_102959.csv, linkedin_jobs_20250816_102959.json\n",
            "[SUMMARY] Total scraped: 120, Unique after dedupe: 120\n",
            "Saved → linkedin_jobs_20250816_102959.csv, linkedin_jobs_20250816_102959.json\n",
            "                     Title                   Company  \\\n",
            "0             Data Analyst                        bp   \n",
            "1             Data Analyst                     Wipro   \n",
            "2             Data Analyst                  Michelin   \n",
            "3  Healthcare Data Analyst        Persistent Systems   \n",
            "4             Data Analyst  The Lubrizol Corporation   \n",
            "\n",
            "                   Location Date Posted  \\\n",
            "0  Pune, Maharashtra, India  2025-07-04   \n",
            "1  Pune, Maharashtra, India  2025-07-18   \n",
            "2  Pune, Maharashtra, India  2025-08-07   \n",
            "3  Pune, Maharashtra, India  2025-08-12   \n",
            "4  Pune, Maharashtra, India  2025-08-02   \n",
            "\n",
            "                                                Link  \\\n",
            "0  https://in.linkedin.com/jobs/view/data-analyst...   \n",
            "1  https://in.linkedin.com/jobs/view/data-analyst...   \n",
            "2  https://in.linkedin.com/jobs/view/data-analyst...   \n",
            "3  https://in.linkedin.com/jobs/view/healthcare-d...   \n",
            "4  https://in.linkedin.com/jobs/view/data-analyst...   \n",
            "\n",
            "                   Scraped At  \n",
            "0  2025-08-16T10:30:02.787006  \n",
            "1  2025-08-16T10:30:02.852530  \n",
            "2  2025-08-16T10:30:02.911039  \n",
            "3  2025-08-16T10:30:02.968296  \n",
            "4  2025-08-16T10:30:03.034586  \n",
            "(120, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# try 5"
      ],
      "metadata": {
        "id": "q4asf3M1cBjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "CONFIG = {\n",
        "    \"keywords\": [\"Data Analyst\", \"Software Engineer\"],\n",
        "    \"locations\": [\"Pune, Maharashtra, India\", \"Bengaluru, Karnataka, India\"],\n",
        "    \"result_limit\": 30,           # Max jobs per keyword-location combo\n",
        "    \"fetch_descriptions\": False,  # Toggle to scrape full job descriptions\n",
        "\n",
        "    # --- FILTERS ---\n",
        "    \"filters\": {\n",
        "        \"remote\": None,           # options: None, \"1\" (remote), \"2\" (on-site), \"3\" (hybrid)\n",
        "        \"date_posted\": \"r86400\", # past week (None, r86400=24h, r604800=week, r2592000=month)\n",
        "        \"experience\": None,       # options: 1=Internship, 2=Entry, 3=Associate, etc.\n",
        "        \"employment_type\": None,  # options: 1=Full-time, 2=Part-time, etc.\n",
        "    },\n",
        "\n",
        "    \"dedupe_on\": [\"Link\"],        # Remove duplicates based on job link\n",
        "    \"save_chunk_size\": 50,        # Save after every N jobs\n",
        "    \"max_retries\": 3,             # Retry failed pages/cards N times\n",
        "    \"retry_delay\": 3              # Seconds between retries\n",
        "}\n",
        "\n",
        "# ==================== DRIVER SETUP ====================\n",
        "def setup_driver() -> uc.Chrome:\n",
        "    options = uc.ChromeOptions()\n",
        "    options.add_argument(\"--headless=new\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = uc.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver\n",
        "\n",
        "# ==================== AUTH ====================\n",
        "def login_linkedin(driver):\n",
        "    li_at = os.getenv(\"LI_AT\")\n",
        "    username = os.getenv(\"LINKEDIN_USER\")\n",
        "    password = os.getenv(\"LINKEDIN_PASS\")\n",
        "\n",
        "    driver.get(\"https://www.linkedin.com/login\")\n",
        "\n",
        "    if li_at:\n",
        "        driver.add_cookie({\"name\": \"li_at\", \"value\": li_at, \"domain\": \".linkedin.com\"})\n",
        "        driver.refresh()\n",
        "        print(\"[INFO] Logged in with li_at cookie\")\n",
        "    elif username and password:\n",
        "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(username)\n",
        "        driver.find_element(By.ID, \"password\").send_keys(password)\n",
        "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
        "        print(\"[INFO] Logged in with username/password\")\n",
        "    else:\n",
        "        print(\"[WARN] No login → scraping public jobs only\")\n",
        "\n",
        "# ==================== URL BUILDER ====================\n",
        "def build_search_url(keyword: str, location: str, start: int = 0, filters: dict = None) -> str:\n",
        "    url = f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}\"\n",
        "    if filters:\n",
        "        if filters.get(\"remote\"): url += f\"&f_WT={filters['remote']}\"\n",
        "        if filters.get(\"date_posted\"): url += f\"&f_TPR={filters['date_posted']}\"\n",
        "        if filters.get(\"experience\"): url += f\"&f_E={filters['experience']}\"\n",
        "        if filters.get(\"employment_type\"): url += f\"&f_JT={filters['employment_type']}\"\n",
        "    return url\n",
        "\n",
        "# ==================== JOB EXTRACTION ====================\n",
        "def extract_job_card(card, fetch_desc: bool, driver, retries=CONFIG[\"max_retries\"]) -> Dict:\n",
        "    \"\"\"Extracts job info from a single LinkedIn job card with retries\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
        "            company = card.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
        "            loc = card.find_element(By.CSS_SELECTOR, \"span.job-search-card__location\").text.strip()\n",
        "            link = card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\").split(\"?\")[0]\n",
        "            date_posted = card.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")\n",
        "\n",
        "            job = {\n",
        "                \"Title\": title,\n",
        "                \"Company\": company,\n",
        "                \"Location\": loc,\n",
        "                \"Date Posted\": date_posted,\n",
        "                \"Link\": link,\n",
        "                \"Scraped At\": datetime.utcnow().isoformat()\n",
        "            }\n",
        "\n",
        "            # Fetch job description if enabled\n",
        "            if fetch_desc:\n",
        "                try:\n",
        "                    card.click()\n",
        "                    WebDriverWait(driver, 5).until(\n",
        "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobs-description\"))\n",
        "                    )\n",
        "                    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "                    desc = soup.select_one(\"div.jobs-description__content\")\n",
        "                    if desc:\n",
        "                        job[\"Job Description\"] = desc.get_text(\" \", strip=True)\n",
        "                except Exception:\n",
        "                    job[\"Job Description\"] = None\n",
        "\n",
        "            return job\n",
        "        except Exception:\n",
        "            time.sleep(CONFIG[\"retry_delay\"])\n",
        "            print(f\"[WARN] Job extraction retry {attempt+1}/{retries}\")\n",
        "    return None\n",
        "\n",
        "# ==================== SCRAPER ====================\n",
        "def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool, filters: dict) -> List[Dict]:\n",
        "    jobs = []\n",
        "    seen_links = set()\n",
        "    start = 0\n",
        "\n",
        "    while len(jobs) < limit:\n",
        "        url = build_search_url(keyword, location, start, filters)\n",
        "\n",
        "        # Retry page load\n",
        "        for attempt in range(CONFIG[\"max_retries\"]):\n",
        "            try:\n",
        "                driver.get(url)\n",
        "                WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"ul.jobs-search__results-list li\"))\n",
        "                )\n",
        "                break\n",
        "            except Exception:\n",
        "                print(f\"[WARN] Page load failed → retry {attempt+1}/{CONFIG['max_retries']}\")\n",
        "                time.sleep(CONFIG[\"retry_delay\"])\n",
        "        else:\n",
        "            print(\"[ERROR] Skipping page due to repeated failures\")\n",
        "            break\n",
        "\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
        "        if not cards:\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            job = extract_job_card(card, fetch_desc, driver)\n",
        "            if job and job[\"Link\"] not in seen_links:\n",
        "                jobs.append(job)\n",
        "                seen_links.add(job[\"Link\"])\n",
        "\n",
        "            if len(jobs) >= limit:\n",
        "                break\n",
        "\n",
        "        start += len(cards)\n",
        "\n",
        "    return jobs\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "def main(CONFIG):\n",
        "    driver = setup_driver()\n",
        "    login_linkedin(driver)\n",
        "\n",
        "    all_jobs = []\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"linkedin_jobs_{ts}.csv\"\n",
        "    json_path = f\"linkedin_jobs_{ts}.json\"\n",
        "\n",
        "    for kw in CONFIG[\"keywords\"]:\n",
        "        for loc in CONFIG[\"locations\"]:\n",
        "            jobs = scrape_jobs(driver, kw, loc, CONFIG[\"result_limit\"], CONFIG[\"fetch_descriptions\"], CONFIG[\"filters\"])\n",
        "            all_jobs.extend(jobs)\n",
        "            print(f\"[INFO] Fetched {len(jobs)} jobs for {kw} in {loc}\")\n",
        "\n",
        "            # Save incrementally\n",
        "            if len(all_jobs) >= CONFIG[\"save_chunk_size\"]:\n",
        "                df = pd.DataFrame(all_jobs)\n",
        "                if CONFIG[\"dedupe_on\"]:\n",
        "                    df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "                df.to_csv(csv_path, index=False)\n",
        "                df.to_json(json_path, orient=\"records\", indent=2)\n",
        "                print(f\"[AUTO-SAVE] Saved {len(df)} jobs → {csv_path}, {json_path}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    # Final save\n",
        "    df = pd.DataFrame(all_jobs)\n",
        "    if CONFIG[\"dedupe_on\"]:\n",
        "        df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    print(f\"[SUMMARY] Total scraped: {len(all_jobs)}, Unique after dedupe: {len(df)}\")\n",
        "    print(f\"Saved → {csv_path}, {json_path}\")\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(CONFIG)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YFEg2LRcFFU",
        "outputId": "a6db2040-3554-41a6-c385-445831c249cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] No login → scraping public jobs only\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Pune, Maharashtra, India\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Bengaluru, Karnataka, India\n",
            "[AUTO-SAVE] Saved 60 jobs → linkedin_jobs_20250816_103524.csv, linkedin_jobs_20250816_103524.json\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Pune, Maharashtra, India\n",
            "[AUTO-SAVE] Saved 88 jobs → linkedin_jobs_20250816_103524.csv, linkedin_jobs_20250816_103524.json\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Bengaluru, Karnataka, India\n",
            "[AUTO-SAVE] Saved 118 jobs → linkedin_jobs_20250816_103524.csv, linkedin_jobs_20250816_103524.json\n",
            "[SUMMARY] Total scraped: 120, Unique after dedupe: 118\n",
            "Saved → linkedin_jobs_20250816_103524.csv, linkedin_jobs_20250816_103524.json\n",
            "                         Title                           Company  \\\n",
            "0                      Analyst                      IntegriChain   \n",
            "1                    Associate                         PwC India   \n",
            "2                 Data Analyst  GK HR Consulting India Pvt. Ltd.   \n",
            "3  Analyst, Clinical Analytics                           Evolent   \n",
            "4                                                                  \n",
            "\n",
            "                   Location Date Posted  \\\n",
            "0  Pune, Maharashtra, India  2025-08-15   \n",
            "1  Pune, Maharashtra, India  2025-08-15   \n",
            "2  Pune, Maharashtra, India  2025-08-16   \n",
            "3  Pune, Maharashtra, India  2025-08-15   \n",
            "4                            2025-08-15   \n",
            "\n",
            "                                                Link  \\\n",
            "0  https://in.linkedin.com/jobs/view/analyst-at-i...   \n",
            "1  https://in.linkedin.com/jobs/view/associate-at...   \n",
            "2  https://in.linkedin.com/jobs/view/data-analyst...   \n",
            "3  https://in.linkedin.com/jobs/view/analyst-clin...   \n",
            "4  https://in.linkedin.com/jobs/view/data-analyst...   \n",
            "\n",
            "                   Scraped At  \n",
            "0  2025-08-16T10:35:27.362958  \n",
            "1  2025-08-16T10:35:27.419264  \n",
            "2  2025-08-16T10:35:27.478970  \n",
            "3  2025-08-16T10:35:27.535919  \n",
            "4  2025-08-16T10:35:27.593099  \n",
            "(118, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# try 6"
      ],
      "metadata": {
        "id": "03RljxxUeOUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "CONFIG = {\n",
        "    \"keywords\": [\"Data Analyst\", \"Software Engineer\"],\n",
        "    \"locations\": [\"Pune, Maharashtra, India\", \"Bengaluru, Karnataka, India\"],\n",
        "    \"result_limit\": 30,           # Max jobs per keyword-location combo\n",
        "    \"fetch_descriptions\": False,  # Toggle to scrape full job descriptions\n",
        "\n",
        "    # --- FILTERS ---\n",
        "    \"filters\": {\n",
        "        \"remote\": None,           # options: None, \"1\" (remote), \"2\" (on-site), \"3\" (hybrid)\n",
        "        \"date_posted\": \"r86400\", # past week (None, r86400=24h, r604800=week, r2592000=month)\n",
        "        \"experience\": None,       # options: 1=Internship, 2=Entry, 3=Associate, etc.\n",
        "        \"employment_type\": None,  # options: 1=Full-time, 2=Part-time, etc.\n",
        "    },\n",
        "\n",
        "    \"dedupe_on\": [\"Link\"],        # Remove duplicates based on job link\n",
        "    \"save_chunk_size\": 50,        # Save after every N jobs\n",
        "    \"max_retries\": 3,             # Retry failed pages/cards N times\n",
        "    \"retry_delay\": 3,             # Seconds between retries\n",
        "\n",
        "    # --- HUMAN-LIKE DELAY ---\n",
        "    \"random_delay\": {\n",
        "        \"enabled\": False,          # True = wait random seconds, False = skip waiting\n",
        "        \"min_sec\": 2,             # Minimum seconds to wait\n",
        "        \"max_sec\": 6              # Maximum seconds to wait\n",
        "    }\n",
        "}\n",
        "\n",
        "# ==================== DRIVER SETUP ====================\n",
        "def setup_driver() -> uc.Chrome:\n",
        "    options = uc.ChromeOptions()\n",
        "    options.add_argument(\"--headless=new\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = uc.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver\n",
        "\n",
        "# ==================== AUTH ====================\n",
        "def login_linkedin(driver):\n",
        "    li_at = os.getenv(\"LI_AT\")\n",
        "    username = os.getenv(\"LINKEDIN_USER\")\n",
        "    password = os.getenv(\"LINKEDIN_PASS\")\n",
        "\n",
        "    driver.get(\"https://www.linkedin.com/login\")\n",
        "\n",
        "    if li_at:\n",
        "        driver.add_cookie({\"name\": \"li_at\", \"value\": li_at, \"domain\": \".linkedin.com\"})\n",
        "        driver.refresh()\n",
        "        print(\"[INFO] Logged in with li_at cookie\")\n",
        "    elif username and password:\n",
        "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(username)\n",
        "        driver.find_element(By.ID, \"password\").send_keys(password)\n",
        "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
        "        print(\"[INFO] Logged in with username/password\")\n",
        "    else:\n",
        "        print(\"[WARN] No login → scraping public jobs only\")\n",
        "\n",
        "# ==================== URL BUILDER ====================\n",
        "def build_search_url(keyword: str, location: str, start: int = 0, filters: dict = None) -> str:\n",
        "    url = f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}\"\n",
        "    if filters:\n",
        "        if filters.get(\"remote\"): url += f\"&f_WT={filters['remote']}\"\n",
        "        if filters.get(\"date_posted\"): url += f\"&f_TPR={filters['date_posted']}\"\n",
        "        if filters.get(\"experience\"): url += f\"&f_E={filters['experience']}\"\n",
        "        if filters.get(\"employment_type\"): url += f\"&f_JT={filters['employment_type']}\"\n",
        "    return url\n",
        "\n",
        "# ==================== RANDOM DELAY ====================\n",
        "def apply_random_delay():\n",
        "    if CONFIG[\"random_delay\"][\"enabled\"]:\n",
        "        delay = random.uniform(CONFIG[\"random_delay\"][\"min_sec\"], CONFIG[\"random_delay\"][\"max_sec\"])\n",
        "        print(f\"[WAIT] Sleeping {delay:.2f} seconds (human-like delay)\")\n",
        "        time.sleep(delay)\n",
        "\n",
        "# ==================== JOB EXTRACTION ====================\n",
        "def extract_job_card(card, fetch_desc: bool, driver, retries=CONFIG[\"max_retries\"]) -> Dict:\n",
        "    \"\"\"Extracts job info from a single LinkedIn job card with retries\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
        "            company = card.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
        "            loc = card.find_element(By.CSS_SELECTOR, \"span.job-search-card__location\").text.strip()\n",
        "            link = card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\").split(\"?\")[0]\n",
        "            date_posted = card.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")\n",
        "\n",
        "            job = {\n",
        "                \"Title\": title,\n",
        "                \"Company\": company,\n",
        "                \"Location\": loc,\n",
        "                \"Date Posted\": date_posted,\n",
        "                \"Link\": link,\n",
        "                \"Scraped At\": datetime.utcnow().isoformat()\n",
        "            }\n",
        "\n",
        "            # Fetch job description if enabled\n",
        "            if fetch_desc:\n",
        "                try:\n",
        "                    card.click()\n",
        "                    WebDriverWait(driver, 5).until(\n",
        "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobs-description\"))\n",
        "                    )\n",
        "                    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "                    desc = soup.select_one(\"div.jobs-description__content\")\n",
        "                    if desc:\n",
        "                        job[\"Job Description\"] = desc.get_text(\" \", strip=True)\n",
        "                except Exception:\n",
        "                    job[\"Job Description\"] = None\n",
        "\n",
        "            return job\n",
        "        except Exception:\n",
        "            time.sleep(CONFIG[\"retry_delay\"])\n",
        "            print(f\"[WARN] Job extraction retry {attempt+1}/{retries}\")\n",
        "    return None\n",
        "\n",
        "# ==================== SCRAPER ====================\n",
        "def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool, filters: dict) -> List[Dict]:\n",
        "    jobs = []\n",
        "    seen_links = set()\n",
        "    start = 0\n",
        "\n",
        "    while len(jobs) < limit:\n",
        "        url = build_search_url(keyword, location, start, filters)\n",
        "\n",
        "        # Retry page load\n",
        "        for attempt in range(CONFIG[\"max_retries\"]):\n",
        "            try:\n",
        "                driver.get(url)\n",
        "                WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"ul.jobs-search__results-list li\"))\n",
        "                )\n",
        "                break\n",
        "            except Exception:\n",
        "                print(f\"[WARN] Page load failed → retry {attempt+1}/{CONFIG['max_retries']}\")\n",
        "                time.sleep(CONFIG[\"retry_delay\"])\n",
        "        else:\n",
        "            print(\"[ERROR] Skipping page due to repeated failures\")\n",
        "            break\n",
        "\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
        "        if not cards:\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            job = extract_job_card(card, fetch_desc, driver)\n",
        "            if job and job[\"Link\"] not in seen_links:\n",
        "                jobs.append(job)\n",
        "                seen_links.add(job[\"Link\"])\n",
        "\n",
        "                # Apply random human-like delay\n",
        "                apply_random_delay()\n",
        "\n",
        "            if len(jobs) >= limit:\n",
        "                break\n",
        "\n",
        "        start += len(cards)\n",
        "\n",
        "    return jobs\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "def main(CONFIG):\n",
        "    driver = setup_driver()\n",
        "    login_linkedin(driver)\n",
        "\n",
        "    all_jobs = []\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"linkedin_jobs_{ts}.csv\"\n",
        "    json_path = f\"linkedin_jobs_{ts}.json\"\n",
        "\n",
        "    for kw in CONFIG[\"keywords\"]:\n",
        "        for loc in CONFIG[\"locations\"]:\n",
        "            jobs = scrape_jobs(driver, kw, loc, CONFIG[\"result_limit\"], CONFIG[\"fetch_descriptions\"], CONFIG[\"filters\"])\n",
        "            all_jobs.extend(jobs)\n",
        "            print(f\"[INFO] Fetched {len(jobs)} jobs for {kw} in {loc}\")\n",
        "\n",
        "            # Save incrementally\n",
        "            if len(all_jobs) >= CONFIG[\"save_chunk_size\"]:\n",
        "                df = pd.DataFrame(all_jobs)\n",
        "                if CONFIG[\"dedupe_on\"]:\n",
        "                    df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "                df.to_csv(csv_path, index=False)\n",
        "                df.to_json(json_path, orient=\"records\", indent=2)\n",
        "                print(f\"[AUTO-SAVE] Saved {len(df)} jobs → {csv_path}, {json_path}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    # Final save\n",
        "    df = pd.DataFrame(all_jobs)\n",
        "    if CONFIG[\"dedupe_on\"]:\n",
        "        df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    print(f\"[SUMMARY] Total scraped: {len(all_jobs)}, Unique after dedupe: {len(df)}\")\n",
        "    print(f\"Saved → {csv_path}, {json_path}\")\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(CONFIG)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URpbqjtleQEI",
        "outputId": "939e1f19-becd-43bd-ce12-8aecf385a11f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] No login → scraping public jobs only\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Pune, Maharashtra, India\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Bengaluru, Karnataka, India\n",
            "[AUTO-SAVE] Saved 60 jobs → linkedin_jobs_20250816_104542.csv, linkedin_jobs_20250816_104542.json\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Pune, Maharashtra, India\n",
            "[AUTO-SAVE] Saved 88 jobs → linkedin_jobs_20250816_104542.csv, linkedin_jobs_20250816_104542.json\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Bengaluru, Karnataka, India\n",
            "[AUTO-SAVE] Saved 118 jobs → linkedin_jobs_20250816_104542.csv, linkedin_jobs_20250816_104542.json\n",
            "[SUMMARY] Total scraped: 120, Unique after dedupe: 118\n",
            "Saved → linkedin_jobs_20250816_104542.csv, linkedin_jobs_20250816_104542.json\n",
            "                                               Title  \\\n",
            "0                                            Analyst   \n",
            "1                                          Associate   \n",
            "2                                       Data Analyst   \n",
            "3  Senior Analyst - Data Analytics - Pan India - ...   \n",
            "4                        Analyst, Clinical Analytics   \n",
            "\n",
            "                            Company                  Location Date Posted  \\\n",
            "0                      IntegriChain  Pune, Maharashtra, India  2025-08-15   \n",
            "1                         PwC India  Pune, Maharashtra, India  2025-08-15   \n",
            "2  GK HR Consulting India Pvt. Ltd.  Pune, Maharashtra, India  2025-08-16   \n",
            "3              Golden Opportunities  Pune, Maharashtra, India  2025-08-16   \n",
            "4                           Evolent                            2025-08-15   \n",
            "\n",
            "                                                Link  \\\n",
            "0  https://in.linkedin.com/jobs/view/analyst-at-i...   \n",
            "1  https://in.linkedin.com/jobs/view/associate-at...   \n",
            "2  https://in.linkedin.com/jobs/view/data-analyst...   \n",
            "3  https://in.linkedin.com/jobs/view/senior-analy...   \n",
            "4  https://in.linkedin.com/jobs/view/analyst-clin...   \n",
            "\n",
            "                   Scraped At  \n",
            "0  2025-08-16T10:45:46.299727  \n",
            "1  2025-08-16T10:45:46.361625  \n",
            "2  2025-08-16T10:45:46.416080  \n",
            "3  2025-08-16T10:45:46.473762  \n",
            "4  2025-08-16T10:45:46.527100  \n",
            "(118, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINALIZED"
      ],
      "metadata": {
        "id": "P16QxM73fhXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "9D-IDg3-fjir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== CONFIG ====================\n",
        "CONFIG = {\n",
        "    \"keywords\": [\"Data Analyst\", \"Software Engineer\"],\n",
        "    \"locations\": [\"Pune, Maharashtra, India\", \"Bengaluru, Karnataka, India\"],\n",
        "    \"result_limit\": 30,           # Max jobs per keyword-location combo\n",
        "    \"fetch_descriptions\": False,  # Toggle to scrape full job descriptions\n",
        "\n",
        "    # --- FILTERS ---\n",
        "    \"filters\": {\n",
        "        \"remote\": None,           # options: None, \"1\" (remote), \"2\" (on-site), \"3\" (hybrid)\n",
        "        \"date_posted\": \"r86400\", # past week (None, r86400=24h, r604800=week, r2592000=month)\n",
        "        \"experience\": None,       # options: 1=Internship, 2=Entry, 3=Associate, etc.\n",
        "        \"employment_type\": None,  # options: 1=Full-time, 2=Part-time, etc.\n",
        "    },\n",
        "\n",
        "    \"dedupe_on\": [\"Link\"],        # Remove duplicates based on job link\n",
        "    \"save_chunk_size\": 50,        # Save after every N jobs\n",
        "    \"max_retries\": 3,             # Retry failed pages/cards N times\n",
        "    \"retry_delay\": 3,             # Seconds between retries\n",
        "\n",
        "    # --- HUMAN-LIKE DELAY ---\n",
        "    \"random_delay\": {\n",
        "        \"enabled\": False,          # True = wait random seconds, False = skip waiting\n",
        "        \"min_sec\": 2,             # Minimum seconds to wait\n",
        "        \"max_sec\": 6              # Maximum seconds to wait\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "VBd02ykfftR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== DRIVER SETUP ====================\n",
        "def setup_driver() -> uc.Chrome:\n",
        "    options = uc.ChromeOptions()\n",
        "    options.add_argument(\"--headless=new\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = uc.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "    return driver"
      ],
      "metadata": {
        "id": "UcEt0t8pfxA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== AUTH ====================\n",
        "def login_linkedin(driver):\n",
        "    li_at = os.getenv(\"LI_AT\")\n",
        "    username = os.getenv(\"LINKEDIN_USER\")\n",
        "    password = os.getenv(\"LINKEDIN_PASS\")\n",
        "\n",
        "    driver.get(\"https://www.linkedin.com/login\")\n",
        "\n",
        "    if li_at:\n",
        "        driver.add_cookie({\"name\": \"li_at\", \"value\": li_at, \"domain\": \".linkedin.com\"})\n",
        "        driver.refresh()\n",
        "        print(\"[INFO] Logged in with li_at cookie\")\n",
        "    elif username and password:\n",
        "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, \"username\"))).send_keys(username)\n",
        "        driver.find_element(By.ID, \"password\").send_keys(password)\n",
        "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
        "        print(\"[INFO] Logged in with username/password\")\n",
        "    else:\n",
        "        print(\"[WARN] No login → scraping public jobs only\")"
      ],
      "metadata": {
        "id": "7HTVHY-af0_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== URL BUILDER ====================\n",
        "def build_search_url(keyword: str, location: str, start: int = 0, filters: dict = None) -> str:\n",
        "    url = f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}\"\n",
        "    if filters:\n",
        "        if filters.get(\"remote\"): url += f\"&f_WT={filters['remote']}\"\n",
        "        if filters.get(\"date_posted\"): url += f\"&f_TPR={filters['date_posted']}\"\n",
        "        if filters.get(\"experience\"): url += f\"&f_E={filters['experience']}\"\n",
        "        if filters.get(\"employment_type\"): url += f\"&f_JT={filters['employment_type']}\"\n",
        "    return url"
      ],
      "metadata": {
        "id": "s7X70QNmf1yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== RANDOM DELAY ====================\n",
        "def apply_random_delay():\n",
        "    if CONFIG[\"random_delay\"][\"enabled\"]:\n",
        "        delay = random.uniform(CONFIG[\"random_delay\"][\"min_sec\"], CONFIG[\"random_delay\"][\"max_sec\"])\n",
        "        print(f\"[WAIT] Sleeping {delay:.2f} seconds (human-like delay)\")\n",
        "        time.sleep(delay)"
      ],
      "metadata": {
        "id": "n5zMojPuf1uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== JOB EXTRACTION ====================\n",
        "def extract_job_card(card, fetch_desc: bool, driver, retries=CONFIG[\"max_retries\"]) -> Dict:\n",
        "    \"\"\"Extracts job info from a single LinkedIn job card with retries\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
        "            company = card.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
        "            loc = card.find_element(By.CSS_SELECTOR, \"span.job-search-card__location\").text.strip()\n",
        "            link = card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\").split(\"?\")[0]\n",
        "            date_posted = card.find_element(By.CSS_SELECTOR, \"time\").get_attribute(\"datetime\")\n",
        "\n",
        "            job = {\n",
        "                \"Title\": title,\n",
        "                \"Company\": company,\n",
        "                \"Location\": loc,\n",
        "                \"Date Posted\": date_posted,\n",
        "                \"Link\": link,\n",
        "                \"Scraped At\": datetime.utcnow().isoformat()\n",
        "            }\n",
        "\n",
        "            # Fetch job description if enabled\n",
        "            if fetch_desc:\n",
        "                try:\n",
        "                    card.click()\n",
        "                    WebDriverWait(driver, 5).until(\n",
        "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobs-description\"))\n",
        "                    )\n",
        "                    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "                    desc = soup.select_one(\"div.jobs-description__content\")\n",
        "                    if desc:\n",
        "                        job[\"Job Description\"] = desc.get_text(\" \", strip=True)\n",
        "                except Exception:\n",
        "                    job[\"Job Description\"] = None\n",
        "\n",
        "            return job\n",
        "        except Exception:\n",
        "            time.sleep(CONFIG[\"retry_delay\"])\n",
        "            print(f\"[WARN] Job extraction retry {attempt+1}/{retries}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "AkAENpg-f1sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== SCRAPER ====================\n",
        "def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool, filters: dict) -> List[Dict]:\n",
        "    jobs = []\n",
        "    seen_links = set()\n",
        "    start = 0\n",
        "\n",
        "    while len(jobs) < limit:\n",
        "        url = build_search_url(keyword, location, start, filters)\n",
        "\n",
        "        # Retry page load\n",
        "        for attempt in range(CONFIG[\"max_retries\"]):\n",
        "            try:\n",
        "                driver.get(url)\n",
        "                WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"ul.jobs-search__results-list li\"))\n",
        "                )\n",
        "                break\n",
        "            except Exception:\n",
        "                print(f\"[WARN] Page load failed → retry {attempt+1}/{CONFIG['max_retries']}\")\n",
        "                time.sleep(CONFIG[\"retry_delay\"])\n",
        "        else:\n",
        "            print(\"[ERROR] Skipping page due to repeated failures\")\n",
        "            break\n",
        "\n",
        "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
        "        if not cards:\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            job = extract_job_card(card, fetch_desc, driver)\n",
        "            if job and job[\"Link\"] not in seen_links:\n",
        "                jobs.append(job)\n",
        "                seen_links.add(job[\"Link\"])\n",
        "\n",
        "                # Apply random human-like delay\n",
        "                apply_random_delay()\n",
        "\n",
        "            if len(jobs) >= limit:\n",
        "                break\n",
        "\n",
        "        start += len(cards)\n",
        "\n",
        "    return jobs"
      ],
      "metadata": {
        "id": "J_hekEBkgQRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== MAIN ====================\n",
        "def main(CONFIG):\n",
        "    driver = setup_driver()\n",
        "    login_linkedin(driver)\n",
        "\n",
        "    all_jobs = []\n",
        "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f\"linkedin_jobs_{ts}.csv\"\n",
        "    json_path = f\"linkedin_jobs_{ts}.json\"\n",
        "\n",
        "    for kw in CONFIG[\"keywords\"]:\n",
        "        for loc in CONFIG[\"locations\"]:\n",
        "            jobs = scrape_jobs(driver, kw, loc, CONFIG[\"result_limit\"], CONFIG[\"fetch_descriptions\"], CONFIG[\"filters\"])\n",
        "            all_jobs.extend(jobs)\n",
        "            print(f\"[INFO] Fetched {len(jobs)} jobs for {kw} in {loc}\")\n",
        "\n",
        "            # Save incrementally\n",
        "            if len(all_jobs) >= CONFIG[\"save_chunk_size\"]:\n",
        "                df = pd.DataFrame(all_jobs)\n",
        "                if CONFIG[\"dedupe_on\"]:\n",
        "                    df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "                df.to_csv(csv_path, index=False)\n",
        "                df.to_json(json_path, orient=\"records\", indent=2)\n",
        "                print(f\"[AUTO-SAVE] Saved {len(df)} jobs → {csv_path}, {json_path}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    # Final save\n",
        "    df = pd.DataFrame(all_jobs)\n",
        "    if CONFIG[\"dedupe_on\"]:\n",
        "        df = df.drop_duplicates(subset=CONFIG[\"dedupe_on\"])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "    print(f\"[SUMMARY] Total scraped: {len(all_jobs)}, Unique after dedupe: {len(df)}\")\n",
        "    print(f\"Saved → {csv_path}, {json_path}\")\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(CONFIG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe9EbfyTgQES",
        "outputId": "3cffd86e-801e-4c00-aefb-912f2cfc1e40"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] No login → scraping public jobs only\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Pune, Maharashtra, India\n",
            "[INFO] Fetched 30 jobs for Data Analyst in Bengaluru, Karnataka, India\n",
            "[AUTO-SAVE] Saved 60 jobs → linkedin_jobs_20250816_105146.csv, linkedin_jobs_20250816_105146.json\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Pune, Maharashtra, India\n",
            "[AUTO-SAVE] Saved 88 jobs → linkedin_jobs_20250816_105146.csv, linkedin_jobs_20250816_105146.json\n",
            "[INFO] Fetched 30 jobs for Software Engineer in Bengaluru, Karnataka, India\n",
            "[AUTO-SAVE] Saved 118 jobs → linkedin_jobs_20250816_105146.csv, linkedin_jobs_20250816_105146.json\n",
            "[SUMMARY] Total scraped: 120, Unique after dedupe: 118\n",
            "Saved → linkedin_jobs_20250816_105146.csv, linkedin_jobs_20250816_105146.json\n",
            "                                               Title  \\\n",
            "0                                            Analyst   \n",
            "1                                          Associate   \n",
            "2                                       Data Analyst   \n",
            "3  Senior Analyst - Data Analytics - Pan India - ...   \n",
            "4                        Analyst, Clinical Analytics   \n",
            "\n",
            "                            Company                  Location Date Posted  \\\n",
            "0                      IntegriChain  Pune, Maharashtra, India  2025-08-15   \n",
            "1                         PwC India  Pune, Maharashtra, India  2025-08-15   \n",
            "2  GK HR Consulting India Pvt. Ltd.  Pune, Maharashtra, India  2025-08-16   \n",
            "3              Golden Opportunities  Pune, Maharashtra, India  2025-08-16   \n",
            "4                           Evolent  Pune, Maharashtra, India  2025-08-15   \n",
            "\n",
            "                                                Link  \\\n",
            "0  https://in.linkedin.com/jobs/view/analyst-at-i...   \n",
            "1  https://in.linkedin.com/jobs/view/associate-at...   \n",
            "2  https://in.linkedin.com/jobs/view/data-analyst...   \n",
            "3  https://in.linkedin.com/jobs/view/senior-analy...   \n",
            "4  https://in.linkedin.com/jobs/view/analyst-clin...   \n",
            "\n",
            "                   Scraped At  \n",
            "0  2025-08-16T10:51:49.465784  \n",
            "1  2025-08-16T10:51:49.534759  \n",
            "2  2025-08-16T10:51:49.599517  \n",
            "3  2025-08-16T10:51:49.664033  \n",
            "4  2025-08-16T10:51:49.724297  \n",
            "(118, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s2zF7WcCgQAy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}