# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AEAQx6YIyofgqa7Ep_x98yCuTKJo-fGY
"""

import os
import time
import random
import pandas as pd
from datetime import datetime
from typing import List, Dict

import streamlit as st
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# ==================== CONFIG DEFAULT ====================
DEFAULT_CONFIG = {
    "keywords": ["Data Analyst"],
    "locations": ["Pune, Maharashtra, India"],
    "result_limit": 20,
    "fetch_descriptions": False,
    "filters": {
        "remote": None,
        "date_posted": None,
        "experience": None,
        "employment_type": None,
    },
    "dedupe_on": ["Link"],
    "save_chunk_size": 50,
    "max_retries": 3,
    "retry_delay": 3,
    "random_delay": {
        "enabled": True,
        "min_sec": 2,
        "max_sec": 5
    }
}

# ==================== DRIVER ====================
def setup_driver() -> uc.Chrome:
    options = uc.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--window-size=1920,1080")
    driver = uc.Chrome(options=options)
    driver.set_page_load_timeout(60)
    driver.set_script_timeout(60)
    return driver

# ==================== LOGIN ====================
def login_linkedin(driver):
    username = os.getenv("LINKEDIN_USER")
    password = os.getenv("LINKEDIN_PASS")

    driver.get("https://www.linkedin.com/login")
    if username and password:
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, "username"))).send_keys(username)
        driver.find_element(By.ID, "password").send_keys(password)
        driver.find_element(By.XPATH, "//button[@type='submit']").click()
        st.info("Logged in with username/password")
    else:
        st.warning("No credentials set ‚Üí scraping public jobs only")

# ==================== URL BUILDER ====================
def build_search_url(keyword: str, location: str, start: int = 0, filters: dict = None) -> str:
    url = f"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start={start}"
    if filters:
        if filters.get("remote"): url += f"&f_WT={filters['remote']}"
        if filters.get("date_posted"): url += f"&f_TPR={filters['date_posted']}"
        if filters.get("experience"): url += f"&f_E={filters['experience']}"
        if filters.get("employment_type"): url += f"&f_JT={filters['employment_type']}"
    return url

# ==================== DELAY ====================
def apply_random_delay(cfg):
    if cfg["random_delay"]["enabled"]:
        delay = random.uniform(cfg["random_delay"]["min_sec"], cfg["random_delay"]["max_sec"])
        st.write(f"‚è≥ Human-like delay {delay:.2f} sec...")
        time.sleep(delay)

# ==================== EXTRACT JOB ====================
def extract_job_card(card, fetch_desc: bool, driver, retries=3) -> Dict:
    for _ in range(retries):
        try:
            title = card.find_element(By.CSS_SELECTOR, "h3").text.strip()
            company = card.find_element(By.CSS_SELECTOR, "h4").text.strip()
            loc = card.find_element(By.CSS_SELECTOR, "span.job-search-card__location").text.strip()
            link = card.find_element(By.TAG_NAME, "a").get_attribute("href").split("?")[0]
            date_posted = card.find_element(By.CSS_SELECTOR, "time").get_attribute("datetime")

            job = {
                "Title": title,
                "Company": company,
                "Location": loc,
                "Date Posted": date_posted,
                "Link": link,
                "Scraped At": datetime.utcnow().isoformat()
            }

            if fetch_desc:
                try:
                    card.click()
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "div.jobs-description"))
                    )
                    soup = BeautifulSoup(driver.page_source, "lxml")
                    desc = soup.select_one("div.jobs-description__content")
                    if desc:
                        job["Job Description"] = desc.get_text(" ", strip=True)
                except Exception:
                    job["Job Description"] = None
            return job
        except Exception:
            time.sleep(2)
    return None

# ==================== SCRAPE ====================
def scrape_jobs(driver, keyword: str, location: str, limit: int, fetch_desc: bool, filters: dict, cfg) -> List[Dict]:
    jobs, seen = [], set()
    start = 0

    while len(jobs) < limit:
        url = build_search_url(keyword, location, start, filters)
        driver.get(url)
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.jobs-search__results-list li"))
            )
        except Exception:
            break

        cards = driver.find_elements(By.CSS_SELECTOR, "ul.jobs-search__results-list li")
        if not cards: break

        for card in cards:
            job = extract_job_card(card, fetch_desc, driver)
            if job and job["Link"] not in seen:
                jobs.append(job)
                seen.add(job["Link"])
                apply_random_delay(cfg)
            if len(jobs) >= limit: break

        start += len(cards)
    return jobs

# ==================== STREAMLIT APP ====================
def main():
    st.title("üíº LinkedIn Job Scraper")

    st.sidebar.header("üîß Search Settings")
    keywords = st.sidebar.text_area("Keywords (comma separated)", "Data Analyst, Software Engineer").split(",")
    locations = st.sidebar.text_area("Locations (comma separated)", "Pune, Maharashtra, India").split(",")
    limit = st.sidebar.slider("Jobs per search", 10, 200, 30)
    fetch_desc = st.sidebar.checkbox("Fetch job descriptions?", False)

    st.sidebar.subheader("ü§ñ Human-like Behavior")
    enable_delay = st.sidebar.checkbox("Enable random delay (safer)", True)

    st.sidebar.subheader("üìå Filters")
    remote = st.sidebar.selectbox("Remote/On-site", [None, "1=Remote", "2=On-site", "3=Hybrid"])
    date_posted = st.sidebar.selectbox("Date Posted", [None, "r86400=24h", "r604800=Week", "r2592000=Month"])
    experience = st.sidebar.selectbox("Experience Level", [None, "1=Internship", "2=Entry", "3=Associate"])
    emp_type = st.sidebar.selectbox("Employment Type", [None, "1=Full-time", "2=Part-time", "3=Contract"])

    if st.button("üöÄ Run Scraper"):
        cfg = DEFAULT_CONFIG.copy()
        cfg["keywords"] = [k.strip() for k in keywords if k.strip()]
        cfg["locations"] = [l.strip() for l in locations if l.strip()]
        cfg["result_limit"] = limit
        cfg["fetch_descriptions"] = fetch_desc
        cfg["random_delay"]["enabled"] = enable_delay
        cfg["filters"] = {
            "remote": remote.split("=")[0] if remote else None,
            "date_posted": date_posted.split("=")[0] if date_posted else None,
            "experience": experience.split("=")[0] if experience else None,
            "employment_type": emp_type.split("=")[0] if emp_type else None
        }

        driver = setup_driver()
        login_linkedin(driver)

        all_jobs = []
        for kw in cfg["keywords"]:
            for loc in cfg["locations"]:
                st.write(f"üîç Scraping: {kw} in {loc}")
                jobs = scrape_jobs(driver, kw, loc, cfg["result_limit"], cfg["fetch_descriptions"], cfg["filters"], cfg)
                all_jobs.extend(jobs)

        driver.quit()

        if all_jobs:
            df = pd.DataFrame(all_jobs)
            df = df.drop_duplicates(subset=cfg["dedupe_on"])
            st.success(f"‚úÖ Scraped {len(df)} jobs")
            st.dataframe(df)

            # Downloads
            os.makedirs("output", exist_ok=True)     ## Ensure output folder exists
            csv = df.to_csv(index=False).encode("utf-8")
            st.download_button("‚¨áÔ∏è Download CSV", csv, "output/linkedin_jobs.csv", "text/csv")
            st.download_button("‚¨áÔ∏è Download JSON", df.to_json(orient="records", indent=2), "output/linkedin_jobs.json")
        else:
            st.error("No jobs found.")

if __name__ == "__main__":
    main()